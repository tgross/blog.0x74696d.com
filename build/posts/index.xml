<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 0x74696d</title>
    <link>https://blog.0x74696d.com/posts/</link>
    <description>Recent content in Posts on 0x74696d</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 13 Mar 2021 08:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://blog.0x74696d.com/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A ZFS Driver for Nomad, Part 4</title>
      <link>https://blog.0x74696d.com/posts/zfs-driver-for-nomad-part4/</link>
      <pubDate>Sat, 13 Mar 2021 08:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/zfs-driver-for-nomad-part4/</guid>
      <description>&lt;p&gt;It&#39;s time to start talking to ZFS, so the first decision I need to
make it whether I want to use a binding to &lt;code&gt;libzfs&lt;/code&gt; or wrap the
command line tools. There&#39;s a &lt;code&gt;libzfs-core&lt;/code&gt; library that&#39;s intended to
be the stable interface to ZFS, but I wasn&#39;t able to find a golang
binding to it and didn&#39;t feel like writing my own. These leaves us
with two main options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mistifyio/go-zfs&#34;&gt;mistifyio/go-zfs&lt;/a&gt;, which wraps
the ZFS CLI.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bicomsystems/go-libzfs&#34;&gt;bicomsystems/go-libzfs&lt;/a&gt;,
which wraps the unstable libzfs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both are permissively licensed. The &lt;code&gt;go-libzfs&lt;/code&gt; library has a fork by
Canonical at &lt;a href=&#34;https://github.com/ubuntu/go-libzfs&#34;&gt;ubuntu/go-libzfs&lt;/a&gt;,
and both seem active which is good news. So first I did a quick spike
with &lt;code&gt;go-libzfs&lt;/code&gt;, which as we&#39;ll see below turned out to be the right
move.&lt;/p&gt;
&lt;p&gt;For the final code listing of the spike see &lt;a href=&#34;https://gist.github.com/tgross/bac0fe124015afec4f68d19a167a6be0&#34;&gt;this
gist&lt;/a&gt;. In
my first pass at building this, I ran into this mysterious error:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ go build -o zfsspike
# github.com/bicomsystems/go-libzfs
../go-libzfs/zpool.go:1108:39: could not determine kind of name for C.pool_initialize_func_t
../go-libzfs/zpool.go:1108:5: could not determine kind of name for C.zpool_initialize
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And that&#39;s the point at which I realized that I need bindings to a
specific version of &lt;code&gt;libzfs&lt;/code&gt;. In my environment I&#39;m not on the
bleeding edge either.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ dpkg -l libzfslinux-dev
Desired=Unknown/Install/Remove/Purge/Hold
| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend
|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)
||/ Name                        Version            Architecture       Description
+++-===========================-==================-==================-============================================================
ii  libzfslinux-dev             0.7.5-1ubuntu16.10 amd64              OpenZFS filesystem development files for Linux
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Fortunately, unlike &lt;code&gt;mistifyio/go-zfs&lt;/code&gt;, the &lt;code&gt;go-libzfs&lt;/code&gt; library has
tags for different versions of ZFS. The tags don&#39;t match the version
of &lt;code&gt;libzfs&lt;/code&gt; you might have, so there&#39;s a bit of experimentation to
find the right version. In my case, I ended up with
&lt;a href=&#34;https://pkg.go.dev/github.com/bicomsystems/go-libzfs@v0.2.3-5&#34;&gt;v.0.2.3-5&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ go build -o zfsspike &amp;amp;&amp;amp; sudo ./zfsspike -name test1
Created zfs dataset rpool/home/tim/test1
         size =&amp;gt; &amp;quot;1614445176&amp;quot;
         free =&amp;gt; &amp;quot;lz4&amp;quot;
    [prop 47] =&amp;gt; &amp;quot;0&amp;quot;
    [prop 63] =&amp;gt; &amp;quot;26624&amp;quot;
         name =&amp;gt; &amp;quot;volume&amp;quot;
       health =&amp;gt; &amp;quot;57344&amp;quot;
    [prop 44] =&amp;gt; &amp;quot;all&amp;quot;
    [prop 58] =&amp;gt; &amp;quot;standard&amp;quot;
...
etc.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;That works, buts adds a new complication to our development cycle: we
need to make sure that the container has access to the appropriate
libraries:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ldd zfsspike
        linux-vdso.so.1 (0x00007ffc741fe000)
        libzfs.so.2 =&amp;gt; /lib/libzfs.so.2 (0x00007ff2551b9000)
        libzpool.so.2 =&amp;gt; /lib/libzpool.so.2 (0x00007ff254c3c000)
        libnvpair.so.1 =&amp;gt; /lib/libnvpair.so.1 (0x00007ff254a29000)
        libpthread.so.0 =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007ff25480a000)
        libc.so.6 =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007ff254419000)
        libzfs_core.so.1 =&amp;gt; /lib/libzfs_core.so.1 (0x00007ff254214000)
        libuutil.so.1 =&amp;gt; /lib/libuutil.so.1 (0x00007ff254002000)
        libm.so.6 =&amp;gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007ff253c64000)
        libblkid.so.1 =&amp;gt; /lib/x86_64-linux-gnu/libblkid.so.1 (0x00007ff253a17000)
        libz.so.1 =&amp;gt; /lib/x86_64-linux-gnu/libz.so.1 (0x00007ff2537fa000)
        librt.so.1 =&amp;gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007ff2535f2000)
        libdl.so.2 =&amp;gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007ff2533ee000)
        /lib64/ld-linux-x86-64.so.2 (0x00007ff255401000)
        libuuid.so.1 =&amp;gt; /lib/x86_64-linux-gnu/libuuid.so.1 (0x00007ff2531e7000)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The Debian Buster container image I was using doesn&#39;t have a libzfs
package at all, and Stretch only has 0.6.5, whereas I&#39;ve got 0.7.x on
my machine. So I&#39;ll start from the same Ubuntu base as my target:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Dockerfile&#34; data-lang=&#34;Dockerfile&#34;&gt;&lt;span class=&#34;k&#34;&gt;FROM&lt;/span&gt;&lt;span class=&#34;s&#34;&gt; ubuntu:bionic&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;RUN&lt;/span&gt; apt-get update &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get install -y libzfslinux-dev&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;COPY&lt;/span&gt; zfsspike /bin/zfsspike&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;After building that image I can test it and get the same results. At
some point I&#39;ll probably want to have a process to release separate
container images for separate versions of &lt;code&gt;libzfs&lt;/code&gt;, and I certainly
don&#39;t want to haul around all of Ubuntu if I don&#39;t have to. We&#39;ll put
a pin in that for later. Our spike has proved the concept, so now to
wire this all up in the CSI plugin.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker run -it --privileged --rm test:latest /bin/zfsspike -name test1
Created zfs dataset rpool/home/tim/test1
     failmode =&amp;gt; &amp;quot;4096&amp;quot;
    [prop 49] =&amp;gt; &amp;quot;0&amp;quot;
    [prop 53] =&amp;gt; &amp;quot;latency&amp;quot;
    [prop 68] =&amp;gt; &amp;quot;18446744073709551615&amp;quot;
    cachefile =&amp;gt; &amp;quot;10485760&amp;quot;
   expandsize =&amp;gt; &amp;quot;off&amp;quot;
    [prop 63] =&amp;gt; &amp;quot;26624&amp;quot;
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Because I like to work clean, I&#39;ll first remove the configuration that
supports the &lt;code&gt;NodeGetInfo&lt;/code&gt; RPC. We discovered last time that&#39;s really
only there to support controllers. The &lt;code&gt;NodeGetInfo&lt;/code&gt; RPC can return a
mostly empty body to make Nomad happy, and I can get rid of the
topology flags. That&#39;s commit
&lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/a594409117d61a2cd6aa43d325db181673c168af&#34;&gt;a594409&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Node RPCs map 1:1 to some specific operations in ZFS, so I&#39;ll
define an interface and implement an empty body for it. Then I&#39;ll want
to call this from the Node RPCs. Remember that &lt;code&gt;NodeStageVolume&lt;/code&gt;
happens when the node first gets the volume, and the
&lt;code&gt;NodePublishVolume&lt;/code&gt; is called when a workload wants to mount it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;kd&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;ZFS&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;interface&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;nf&#34;&gt;Create&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;ZFSCreateOptions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;error&lt;/span&gt;
    &lt;span class=&#34;nf&#34;&gt;Mount&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;ZFSMountOptions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;error&lt;/span&gt;
    &lt;span class=&#34;nf&#34;&gt;Unmount&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;ZFSUnmountOptions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;error&lt;/span&gt;
    &lt;span class=&#34;nf&#34;&gt;Destroy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;ZFSDestroyOptions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;error&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I&#39;m using an interface here because creating and destroying datasets
requires root privileges and modifies the host environment. So if I
want to ever test the rest of the code, I&#39;ll want to be able to swap
that out for a side-effect free mock. Also, at some point I&#39;ll want
the ZFS client to be able to perform a bunch of other operations that
don&#39;t map to Node RPCs, so having the RPC code depend on a smaller
slice of interface reduces the size of whatever test mocks I&#39;ll need.&lt;/p&gt;
&lt;p&gt;That&#39;s all committed as
&lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/de756510ecc8af168329e80616bb323c7fa61cc0&#34;&gt;de75651&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next I want to start creating datasets, but I need to figure out how
to configure all the parameters. There&#39;s going to be 3 sources of
configuration for every mounted dataset:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Configuration we set for the plugin. This could be passed as
configuration flags or a config file for the plugin. The cluster
operator is the one who sets these configs.&lt;/li&gt;
&lt;li&gt;Configuration we set when registering the volume as part of the
&lt;code&gt;VolumeContext&lt;/code&gt;. These values could come from either the cluster
operator or the job submitter, if they have &lt;code&gt;write-volume&lt;/code&gt; too.&lt;/li&gt;
&lt;li&gt;Configuration we set when mounting the volume. This is set on a
per-job basis, so should be limited to those options that can change
between mounts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I don&#39;t want job submitters to control the root dataset path or the
maximum size (quota) of a dataset, and I&#39;ll probably want to provide a
configurable default quota. So I&#39;ll wire up configuration for those
values and provide some basic validation that the root ZFS dataset
exists. That&#39;s
&lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/f4bdf44d9709ccc12f6a71c03311cb0458041564&#34;&gt;f4bdf44&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Next I&#39;m going to look at what parameters the
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#nodestagevolume&#34;&gt;&lt;code&gt;NodeStageVolume&lt;/code&gt;&lt;/a&gt;
and
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#nodepublishvolume&#34;&gt;&lt;code&gt;NodePublishVolume&lt;/code&gt;&lt;/a&gt;
RPCs are going to give me.&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;NodeStageVolume&lt;/code&gt; I&#39;ve got a volume ID and a staging path
controlled by the orchestrator. This staging path is where it should
be mounted during staging but we can safely ignore that for ZFS where
we can stage without mounting. The publish context field is only used
if there&#39;s a controller, so I can safely ignore that. Secrets are of
no use here either.&lt;/p&gt;
&lt;p&gt;The volume capability is a little hairy to unpack: it has a bunch of
structs with empty bodies currently used as flags (presumably because
the spec authors expect to fill them out in the future). So I&#39;ll have
to nil-check those. I&#39;m planning on only exposing ZFS &lt;code&gt;filesystem&lt;/code&gt;
datasets and not &lt;code&gt;volume&lt;/code&gt;, so I&#39;ll validate that the request isn&#39;t
asking for a raw block volume. And don&#39;t want to let the user set the
a filesystem type, because it&#39;s always ZFS.&lt;/p&gt;
&lt;p&gt;But I also want to let the user pass mount flags. These will show up
as a slice of strings. The volume context is a map of properties that
the orchestrator will pass to the mount step from when the volume was
created. In Nomad we&#39;ve differentiated between creating the volume and
registering it, which allows us to set all the creation parameters
even for plugins that don&#39;t support the Controller&#39;s &lt;code&gt;CreateVolume&lt;/code&gt;
RPC. So I want to pass the parameters that we recorded during volume
registration as a map to the plugin. I&#39;ll use these to set the
per-volume quota and any other option that can&#39;t be changed after the
dataset is created. They should override any mount flags. I&#39;ll parse
these fields from the request and add it to the create options struct.&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;NodePublishVolume&lt;/code&gt; I have most of the same fields, with the extra
quirk of the &amp;quot;read only&amp;quot; field and a target path, which is what we
really care about for making available for the workload. And lastly
for the &lt;code&gt;NodeUnpublishVolume&lt;/code&gt; and &lt;code&gt;NodeUnstageVolume&lt;/code&gt; RPCs there&#39;s
just the name and path to deal with.&lt;/p&gt;
&lt;p&gt;I don&#39;t want the ZFS client to have to worry about serialization
concerns, so I&#39;ll do some basic validation of the inputs and ditch the
protobuffers in the RPC handlers before passing down options into the
ZFS client. This is all committed as
&lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/5cf96ace406a0684492a88420acf497da62487c0&#34;&gt;5cf96ac&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now that I&#39;ve got those parameters, how do I pass them to the ZFS
client?&lt;/p&gt;
&lt;p&gt;ZFS supports &lt;em&gt;lots&lt;/em&gt; of properties (see
&lt;a href=&#34;https://openzfs.github.io/openzfs-docs/man/8/zfsprops.8.html&#34;&gt;&lt;code&gt;zfsprops(8)&lt;/code&gt;&lt;/a&gt;). These
get passed as a map to the
&lt;a href=&#34;https://pkg.go.dev/github.com/bicomsystems/go-libzfs#DatasetCreate&#34;&gt;&lt;code&gt;DatasetCreate&lt;/code&gt;&lt;/a&gt;
function, but they&#39;re typed a little unexpectedly:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;nx&#34;&gt;props&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;make&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;zfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Prop&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;zfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Property&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nx&#34;&gt;props&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;zfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;DatasetPropVolsize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;zfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Property&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;strSize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The library doesn&#39;t have a set of string constants that map to the
properties, because this is wrapping &lt;code&gt;libzfs&lt;/code&gt; and not the command line
tools. Unfortunately although they have a
&lt;a href=&#34;https://pkg.go.dev/github.com/bicomsystems/go-libzfs#DatasetPropertyToName&#34;&gt;&lt;code&gt;DatasetPropertyToName&lt;/code&gt;&lt;/a&gt;
function they don&#39;t have the reverse. You can create whatever mapping
you&#39;d like and the library &amp;quot;resolves&amp;quot; them into the real
properties. This turns out to make debugging during development a
little challenging if you&#39;ve got a lot of properties; if the property
doesn&#39;t successfully resolve it won&#39;t have a name you can
&lt;code&gt;fmt.Printf&lt;/code&gt;, just a number to look up. In any case, there&#39;ll be a bit
of code to map the inputs to those properties.&lt;/p&gt;
&lt;p&gt;On top of that, I don&#39;t want the user to be able to set properties
that are read-only to the operator, like &amp;quot;mounted&amp;quot;, so I&#39;ll need to
filter all those out.&lt;/p&gt;
&lt;p&gt;First I&#39;ll sketch out a &lt;code&gt;Create&lt;/code&gt; method that gives me an idea of the shape of the API:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;z&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;ZFSClient&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;Create&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;opts&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;ZFSCreateOptions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;error&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;dcPath&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;z&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;opts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;props&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;err&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;z&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;createProperties&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;opts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;StagingPath&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;opts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;opts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;MountFlags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;err&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;!=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;nil&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;fmt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Errorf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;validation error: %v&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;err&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

    &lt;span class=&#34;nx&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;err&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;zfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;DatasetCreate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;dcPath&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;zfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;DatasetTypeFilesystem&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;props&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;err&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;!=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;nil&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;err&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Close&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;nil&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;z&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;ZFSClient&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;createProperties&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;stagingPath&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;params&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;mountFlags&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;kd&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;zfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Prop&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;zfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Property&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;error&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;

    &lt;span class=&#34;nx&#34;&gt;props&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;zfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Prop&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;zfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Property&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{}&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;props&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;nil&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So I know I&#39;ll most likely end up with a giant switch statement that
validates the parameters and merges them on top of the plugin
defaults. So far almost everything in the code has been &amp;quot;plumbing&amp;quot;:
serializing and deserializing, shuffling protobufs around, setting up
config, etc. Nothing worth writing unit tests for. But now that I&#39;ve
got some logic to worry about, I&#39;ll start writing some tests. Note
that for unit testing, I don&#39;t want to have real ZFS datasets created,
so I need to keep the validation logic cleanly factored away from the
side-effects.&lt;/p&gt;
&lt;p&gt;I start with a failing test like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;TestZFS_CreateProperties&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;testing&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;zclient&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;testZFSClient&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;require&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;NotNil&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;zclient&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;nx&#34;&gt;testCases&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;desc&lt;/span&gt;        &lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;params&lt;/span&gt;      &lt;span class=&#34;kd&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;mountFlags&lt;/span&gt;  &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;expected&lt;/span&gt;    &lt;span class=&#34;kd&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;zfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Prop&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;zfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Property&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;expectedErr&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;error&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}{&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
            &lt;span class=&#34;nx&#34;&gt;desc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;quota parameter overrides default&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
            &lt;span class=&#34;nx&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
                &lt;span class=&#34;s&#34;&gt;&amp;#34;quota&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;30M&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
            &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
            &lt;span class=&#34;nx&#34;&gt;mountFlags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{},&lt;/span&gt;
            &lt;span class=&#34;nx&#34;&gt;expected&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;zfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Prop&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;zfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Property&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
                &lt;span class=&#34;nx&#34;&gt;zfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;DatasetPropQuota&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;zfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Property&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;30M&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
            &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
            &lt;span class=&#34;nx&#34;&gt;expectedErr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;nil&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;tc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;range&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;testCases&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;tc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;tc&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;tc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;desc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;testing&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
            &lt;span class=&#34;nx&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Parallel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;

            &lt;span class=&#34;nx&#34;&gt;props&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;err&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;zclient&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;createProperties&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;tc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;desc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;tc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;tc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;mountFlags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;tc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;expectedErr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;!=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;nil&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
                &lt;span class=&#34;nx&#34;&gt;require&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;EqualError&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;err&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;tc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;expectedErr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Error&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
            &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
                &lt;span class=&#34;nx&#34;&gt;require&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;NoError&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;err&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
                &lt;span class=&#34;nx&#34;&gt;require&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;EqualValues&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;tc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;expected&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;props&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In my first attempt I tried to generate a string-to-ZFS properties
function to reverse &lt;code&gt;DatasetPropertyToName&lt;/code&gt;, only to realize the pool
properties and dataset properties were implemented as the same set,
but without distinct names. This resulted in a big old switch
statement with duplicate keys, so I had to toss that and just
hand-roll a function for the properties I care about.&lt;/p&gt;
&lt;p&gt;There&#39;s a crapton of code here for parsing the configuration,
parameters, etc. Once I&#39;ve got most of this, it becomes obvious that
the &lt;code&gt;Create&lt;/code&gt; isn&#39;t going to use the &lt;code&gt;mountFlags&lt;/code&gt;, so I toss those out
and keep them only for the &lt;code&gt;Mount&lt;/code&gt; method that gets called from the
&lt;code&gt;NodePublishVolume&lt;/code&gt; RPC.&lt;/p&gt;
&lt;p&gt;Then I got a little stuck when looking at &lt;a href=&#34;https://pkg.go.dev/github.com/bicomsystems/go-libzfs@v0.2.3-5#Dataset.Mount&#34;&gt;this
method&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;d&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;Mount&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;options&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;flags&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;err&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;error&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The flags aren&#39;t documented in &lt;code&gt;go-libzfs&lt;/code&gt;, so I had to do a little
spelunking in the ZFS source code. The
&lt;a href=&#34;https://github.com/openzfs/zfs/blob/zfs-0.7.5/lib/libspl/include/sys/mount.h#L89&#34;&gt;&lt;code&gt;mount.h&lt;/code&gt;&lt;/a&gt;
header file defines them. Later versions of this code add encryption
flags, but I can&#39;t test those on the environment where I&#39;m working so
I&#39;m going to leave these out for now.&lt;/p&gt;
&lt;p&gt;All that mess is commit
&lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/3ab18f1691b988829803f66781efa9af26714158&#34;&gt;3ab18f1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;At this point I decide I want to run that build on Nomad so I can do
some testing with it. We saw earlier that we needed a new base to work
with. That&#39;s
&lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/e1dca208753fe90d863143b33ddb0ecdcd43000e&#34;&gt;e1dca20&lt;/a&gt;
for now, but I&#39;d like to revisit that with a leaner container image
later on.&lt;/p&gt;
&lt;p&gt;Unfortunately the development cycle of spinning up Nomad, running the
plugin, registering the volume, and deploying a job that wants that
volume is a little longer than I&#39;d like. So I ended up temporarily
hacking in a testing flag option like the one below (in
&lt;code&gt;main.go&lt;/code&gt;). That&#39;s not committed but I&#39;ll want to come back to revisit
it for integration testing later:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;cfg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;testing&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;err&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;zclient&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Create&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;ZFSCreateOptions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;Name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;        &lt;span class=&#34;s&#34;&gt;&amp;#34;test-volume-0&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;StagingPath&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;/csi/staging/test-volume[0]/rw-file-system-single-node-writer&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;MountFlags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;  &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;readonly&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;-O&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;Params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;      &lt;span class=&#34;kc&#34;&gt;nil&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;})&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;err&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;!=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;nil&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;fmt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Println&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;err&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;After a bit of testing I find some bugs. There are some default properties that weren&#39;t compatible with my environment. And I forgot to defer closing the dataset in the &lt;code&gt;Mount&lt;/code&gt;, &lt;code&gt;Unmount&lt;/code&gt;, and &lt;code&gt;Destroy&lt;/code&gt; methods. Those fixes are &lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/84cc719d5a6a3f6a3d207fbe6f5d88411065e116&#34;&gt;84cc719&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;At this point I was ready to work up an example job for consuming the
volume to test the plugin end-to-end. That&#39;s when I realized the
plugin job specification wasn&#39;t marked with &lt;code&gt;privileged = true&lt;/code&gt;, even
though I was the one to write the giant warning about that on the
Nomad
&lt;a href=&#34;https://www.nomadproject.io/docs/job-specification/csi_plugin#csi_plugin-parameters&#34;&gt;&lt;code&gt;csi_plugin&lt;/code&gt;&lt;/a&gt;
parameters documentation. How embarrassing! That&#39;s in
&lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/f0fd028663ec382826e527543f5516a7ee2f08a2&#34;&gt;f0fd028&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;At this point I get the following error when Nomad&#39;s CSI hook tries to mount the volume:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;2021-03-13T13:30:55-05:00  Driver Failure   failed to create container: API error (400): invalid mount config for type &amp;quot;bind&amp;quot;: bind source path does not exist: /tmp/NomadClient194408044/csi/node/csi-zfs/per-alloc/b7995d2d-5afc-bcf3-d171-e23767ccaeff/test-volume[0]/rw-file-system-single-node-writer&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This path looks ridiculous but it breaks down to
&lt;code&gt;${nomad_data_dir}/csi/node/${plugin_id}/per-alloc/${alloc_id}/${volume_id}/${mount_type}&lt;/code&gt;. If
I add some
&lt;a href=&#34;https://pkg.go.dev/github.com/davecgh/go-spew/spew&#34;&gt;&lt;code&gt;go-spew&lt;/code&gt;&lt;/a&gt;
debugging, I see the following in the plugin&#39;s allocation logs:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nomad alloc logs 1fd
(*main.ZFSMountOptions)(0xc00010fe00)({
 Name: (string) (len=13) &amp;quot;test-volume-0&amp;quot;,
 StagingPath: (string) (len=61) &amp;quot;/csi/staging/test-volume[0]/rw-file-system-single-node-writer&amp;quot;,
 TargetPath: (string) (len=100) &amp;quot;/csi/per-alloc/fa664bfb-30a5-3104-9999-b6defe5e1c41/test-volume[0]/rw-file-system-single-node-writer&amp;quot;,
 MountFlags: ([]string) (len=2 cap=2) {
  (string) (len=8) &amp;quot;readonly&amp;quot;,
  (string) (len=2) &amp;quot;-O&amp;quot;
 },
 Params: (map[string]string) &amp;lt;nil&amp;gt;
})
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;That made me realize we hadn&#39;t yet made the target path the dataset&#39;s
mount point, so that&#39;s done in
&lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/654140a5f7914be78ebd670db9055a0a2584e42f&#34;&gt;654140a&lt;/a&gt;. Now
when I test the example job, I can see the dataset get created:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ zfs list | grep test-volume
rpool/home/tim/test-volume-0                                                               96K   355G    96K  /csi/per-alloc/ca4c08e7-eaab-5ebd-6582-1ef254a39b77/test-volume[0]/rw-file-system-single-node-writer
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If I exec into the job&#39;s allocation and write to the location in the
&lt;code&gt;volume_mount&lt;/code&gt;, I can take a ZFS snapshot of it, clone that snapshot,
and verify that the dataset had been properly mounted to the
container:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nomad alloc exec ca4 /bin/sh
/ # touch /srv/test.txt
/ # exit

$ sudo zfs snapshot rpool/home/tim/test-volume-0@test1
$ sudo zfs clone rpool/home/tim/test-volume-0@test1  rpool/home/tim/restored
$ ls ~/restored
test.txt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;At this point if I try to stop the job, I&#39;ll get the following error
in the Nomad client logs:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;2021-03-13T13:50:30.474-0500 [ERROR] client.alloc_runner: postrun
failed: alloc_id=ca4c08e7-eaab-5ebd-6582-1ef254a39b77 error=&amp;quot;hook
&amp;quot;csi_hook&amp;quot; failed: 1 error occurred: rpc error: could not detach
from node: node detach volume: rpc error: code = Unknown desc =
Cannot destroy dataset rpool/home/tim/test-volume-0: filesystem has
children&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That&#39;s because the dataset has child datasets because of my
snapshot. I&#39;ll have to clean that up manually for the moment, but
that&#39;ll be something to figure out when I implement the snapshot
workflow.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo zfs destroy rpool/home/tim/test-volume-0
cannot destroy &#39;rpool/home/tim/test-volume-0&#39;: filesystem has children
use &#39;-r&#39; to destroy the following datasets:
rpool/home/tim/test-volume-0@test1
$ sudo zfs destroy -r rpool/home/tim/test-volume-0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;At this point the plugin can create, mount, unmount, and delete ZFS
datasets. But right now we&#39;re destroying every dataset as soon as
we&#39;re done with it! And we haven&#39;t yet touched upon migrating the
datasets between nodes. In the next post in this series, I&#39;ll work
through a design for coordinating the plugins for peer-to-peer backup
and migration of data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A ZFS Driver for Nomad, Part 3</title>
      <link>https://blog.0x74696d.com/posts/zfs-driver-for-nomad-part3/</link>
      <pubDate>Sun, 21 Feb 2021 08:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/zfs-driver-for-nomad-part3/</guid>
      <description>&lt;p&gt;Last time we implemented the Container Storage Interface (CSI)
&lt;strong&gt;Identity&lt;/strong&gt; service, so now it&#39;s time to look at the &lt;strong&gt;Controller&lt;/strong&gt; and
&lt;strong&gt;Node&lt;/strong&gt; services. The CSI spec has a lot of detail as to the
protocol, but leaves a lot of the intentions behind each service left
unsaid for the implementer to discover.&lt;/p&gt;
&lt;p&gt;The spec tells us that
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#node-service-rpc&#34;&gt;Node&lt;/a&gt;
service RPCs shall run on the node where the volume is used. There are
only a few required RPCs: &lt;code&gt;NodeGetCapabilities&lt;/code&gt; that tells us which
RPCs are implemented, and &lt;code&gt;NodePublishVolume&lt;/code&gt; and
&lt;code&gt;NodeUnpublishVolume&lt;/code&gt; that are call when the orchestrator has a
workload that wants to use the volume. The &lt;code&gt;NodeStageVolume&lt;/code&gt; and
&lt;code&gt;NodeUnstageVolume&lt;/code&gt; are optional, and are for preparing the volume for
its first use on the node where the volume is used. These definitions
are pretty vague, but that&#39;s required for the CSI authors to cover a
very broad range of storage providers.&lt;/p&gt;
&lt;p&gt;The optional
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#controller-service-rpc&#34;&gt;Controller&lt;/a&gt;
service RPCs include &lt;code&gt;CreateVolume&lt;/code&gt;, &lt;code&gt;ControllerPublishVolume&lt;/code&gt;, and
&lt;code&gt;CreateSnapshot&lt;/code&gt;, so at first glance this sounds like I&#39;ll want this
for the ZFS plugin. But the Controller RPCs differ in one important
respect: they don&#39;t necessarily happen on the same node where we&#39;re
going to use the volume! Unfortunately the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/developing.html&#34;&gt;Kubernetes CSI development
guide&lt;/a&gt; doesn&#39;t
describe the purpose of the Controller, but we can piece it together
from the requirements of its component RPCs.&lt;/p&gt;
&lt;p&gt;The Controller service is for workflows with the storage provider
infrastructure APIs, whereas the Node service is for workflows
specific to a given host. If we use AWS Elastic Block Storage (EBS)
volumes as an example, the Controller service tells AWS to create the
EBS volume (&lt;code&gt;CreateVolume&lt;/code&gt;) and attach the volume to the EC2 virtual
machine (&lt;code&gt;ControllerPublishVolume&lt;/code&gt;), whereas the Node service formats
the volume (&lt;code&gt;NodeStageVolume&lt;/code&gt;) and mounts it for the container
(&lt;code&gt;NodePublishVolume&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;In the case of the ZFS plugin, there&#39;s no &amp;quot;remote ZFS API&amp;quot;. I&#39;ll
ultimately need some sort of service to store the snapshots that I
&lt;code&gt;zfs send&lt;/code&gt;, but all the workflows for this will need to be driven from
the host where the volume is in use. So I can drop the Controller
service entirely.&lt;/p&gt;
&lt;p&gt;With that in mind, I want to get the plugin to the point where it
registers as healthy with Nomad. The last error we got was because
the &lt;code&gt;NodeGetInfo&lt;/code&gt; service wasn&#39;t implemented:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;2021-02-15T15:47:33.839-0500 [WARN] client.csi-zfs: finished client
unary call: grpc.code=Unimplemented duration=357.548s
grpc.service=csi.v1.Node grpc.method=NodeGetInfo&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;First, I&#39;ll do a bit of refactoring and move the Identity service into
its own file. I can remove the controller capability from
&lt;code&gt;GetPluginCapabilities&lt;/code&gt;, and I can change out the &amp;quot;monolith&amp;quot; for
&amp;quot;node&amp;quot; in my jobspec for the plugin. Next I&#39;ll implement the
&lt;a href=&#34;https://pkg.go.dev/github.com/container-storage-interface/spec@v1.3.0/lib/go/csi#NodeServer&#34;&gt;&lt;code&gt;NodeServer&lt;/code&gt;&lt;/a&gt;
interface, just by dropping a bunch of empty function bodies in a new
&lt;code&gt;node.go&lt;/code&gt; file. All that is commit
&lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/b77e1fe20cb5e58411bcc09a28d87a81815a4aee&#34;&gt;b77e1fe&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I compile and run that, but still get the same error. So I check the
allocation logs and find the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ERROR: 2021/02/21 18:17:22 [core] grpc: server failed to encode
response: rpc error: code = Internal desc = grpc: error while
marshaling: proto: Marshal called with nil&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&#39;s look at the spec for
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetinfo&#34;&gt;&lt;code&gt;NodeGetInfo&lt;/code&gt;&lt;/a&gt;
and the
&lt;a href=&#34;https://pkg.go.dev/github.com/container-storage-interface/spec@v1.3.0/lib/go/csi#NodeGetInfoResponse&#34;&gt;&lt;code&gt;NodeGetInfoResponse&lt;/code&gt;&lt;/a&gt;
in a little more detail.&lt;/p&gt;
&lt;p&gt;The fields look oriented towards cloud storage use cases, where cloud
vendor storage volumes will be attached to cloud vendor VMs over cloud
vendor networks. We can see this comment for the &lt;code&gt;node_id&lt;/code&gt; field:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// The identifier of the node as understood by the SP.
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// This field is REQUIRED.
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// This field MUST contain enough information to uniquely identify
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// this specific node vs all other nodes supported by this plugin.
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// This field SHALL be used by the CO in subsequent calls, including
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// `ControllerPublishVolume`, to refer to this node.
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// The SP is NOT responsible for global uniqueness of node_id across
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// multiple SPs.
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Well using the hostname probably isn&#39;t a great option here. I&#39;ll let
the operator set this value, assuming they&#39;ll want to use the Nomad
node ID via &lt;a href=&#34;https://www.nomadproject.io/docs/runtime/interpolation&#34;&gt;attribute
interpolation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For the &lt;code&gt;max_volumes_per_node&lt;/code&gt; field, I see:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// Maximum number of volumes that controller can publish to the node.
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Curious.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; Well, we have no controller. So I can skip this.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Topology&lt;/code&gt; works out to be a map of strings. We could have these
automatically pick up values from the environment Nomad provides, but
in the interest of portability, I&#39;ll have the user provide these as a
list of &lt;code&gt;--topology&lt;/code&gt; flags. I&#39;m building up enough config now that
I&#39;ll pull that out into its own struct, file, and a &lt;code&gt;newConfig&lt;/code&gt;
function that hides it away from us. That commit is
&lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/128b9365a53725be346dd84cd28bae79e82fc7ec&#34;&gt;128b936&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But wait a sec... what&#39;s this comment at the top of &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetinfo&#34;&gt;&lt;code&gt;NodeGetInfo&lt;/code&gt;&lt;/a&gt;?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A Node Plugin MUST implement this RPC call if the plugin has
&lt;code&gt;PUBLISH_UNPUBLISH_VOLUME&lt;/code&gt; controller capability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We &lt;em&gt;don&#39;t&lt;/em&gt; have that capability, as we removed it earlier. So that RPC
shouldn&#39;t be getting hit. It&#39;s a bug in Nomad! When the Nomad client
fingerprints the plugin, it checks &lt;code&gt;NodeGetInfo&lt;/code&gt; if
&lt;a href=&#34;https://github.com/hashicorp/nomad/blob/v1.0.3/client/pluginmanager/csimanager/fingerprint.go#L110-L111&#34;&gt;&lt;code&gt;p.fingerprintNode&lt;/code&gt;&lt;/a&gt;
is set, which is set if the plugin is of type
&lt;a href=&#34;https://github.com/hashicorp/nomad/blob/v1.0.3/client/pluginmanager/csimanager/instance.go#L58&#34;&gt;&lt;code&gt;PluginCSITypeNode&lt;/code&gt;&lt;/a&gt;. That
check for &lt;code&gt;NodeGetInfo&lt;/code&gt; should be for &lt;code&gt;p.fingerprintController&lt;/code&gt;, not
&lt;code&gt;p.fingerprintNode&lt;/code&gt;. Oops! I&#39;ll take a quick detour to open
&lt;a href=&#34;https://github.com/hashicorp/nomad/issues/10055&#34;&gt;nomad/#10055&lt;/a&gt;, but
in the meantime we&#39;ll keep our implementation in place and we can rip
it out once that&#39;s been fixed in Nomad.&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;NodeGetInfo&lt;/code&gt; implemented, I compile and run on Nomad again. I
see the client starts it with all the arguments I&#39;d expect, but now I
get an error for &lt;code&gt;NodeGetCapabilities&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;2021-02-21T14:15:28.181-0500 [WARN] client.csi-zfs: finished client
unary call: grpc.code=Internal duration=2.401155ms
grpc.service=csi.v1.Node grpc.method=NodeGetCapabilities&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Armed with the
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetcapabilities&#34;&gt;&lt;code&gt;NodeGetCapabilities&lt;/code&gt;&lt;/a&gt;
spec and the
&lt;a href=&#34;https://pkg.go.dev/github.com/container-storage-interface/spec@v1.3.0/lib/go/csi#NodeGetCapabilitiesResponse&#34;&gt;&lt;code&gt;NodeGetCapabilitiesResponse&lt;/code&gt;&lt;/a&gt;
doc, I can work up an empty response body:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-diff&#34; data-lang=&#34;diff&#34;&gt; func (n *NodeServer) NodeGetCapabilities(context.Context, *csipb.NodeGetCapabilitiesRequest) (
        *csipb.NodeGetCapabilitiesResponse, error) {
&lt;span class=&#34;gd&#34;&gt;-       return nil, nil
&lt;/span&gt;&lt;span class=&#34;gd&#34;&gt;&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       return &amp;amp;csipb.NodeGetCapabilitiesResponse{}, nil
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;&lt;/span&gt; }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That silences the error:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;2021-02-21T14:20:21.541-0500 [DEBUG] client: detected new CSI plugin: name=csi-zfs type=csi-node&lt;/p&gt;
&lt;p&gt;2021-02-21T14:20:21.543-0500 [DEBUG] client.csi-zfs: volume manager setup complete&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But this endpoint is what advertises the plugins capabilities, so I&#39;ll
return the capabilities instead:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-diff&#34; data-lang=&#34;diff&#34;&gt; func (n *NodeServer) NodeGetCapabilities(context.Context, *csipb.NodeGetCapabilitiesRequest) (
        *csipb.NodeGetCapabilitiesResponse, error) {
&lt;span class=&#34;gd&#34;&gt;-       return nil, nil
&lt;/span&gt;&lt;span class=&#34;gd&#34;&gt;&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       return &amp;amp;csipb.NodeGetCapabilitiesResponse{
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+               Capabilities: []*csipb.NodeServiceCapability{
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                       {
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                               Type: &amp;amp;csipb.NodeServiceCapability_Rpc{
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                                       Rpc: &amp;amp;csipb.NodeServiceCapability_RPC{
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                                               Type: csipb.NodeServiceCapability_RPC_STAGE_UNSTAGE_VOLUME,
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                                       },
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                               },
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                       },
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+               },
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       }, nil
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;&lt;/span&gt; }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If I build and run the new plugin, it registers itself and is marked
as healthy by the Nomad server, ready to publish volumes. That&#39;s
commit
&lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/02ef228886042f3f524d2a8429829b15bd94f53e&#34;&gt;02ef228&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;$ nomad plugin status csi-zfs
ID                   = csi-zfs
Provider             = zfs.csi.0x74696d.com
Version              = 0.0.1
Controllers Healthy  = 0
Controllers Expected = 0
Nodes Healthy        = 1
Nodes Expected       = 1

Allocations
ID        Node ID   Task Group  Version  Desired  Status   Created  Modified
f73abbe3  9c559244  plugin      0        run      running  16s ago  4s ago
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once I get into performing ZFS workflows, I&#39;m going to want to observe
them. So one last item before I wrap up today&#39;s work is to add some
logging. I&#39;m fond of the API for
&lt;a href=&#34;https://pkg.go.dev/github.com/apex/log&#34;&gt;&lt;code&gt;apex/log&lt;/code&gt;&lt;/a&gt;, so I&#39;ll configure a
logger in &lt;code&gt;config.go&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-diff&#34; data-lang=&#34;diff&#34;&gt;&lt;span class=&#34;gi&#34;&gt;+       var logLevel = flag.String(&amp;#34;log-level&amp;#34;, &amp;#34;debug&amp;#34;, `Logging level. One of:
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+debug, info, warn, error, fatal`)
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;&lt;/span&gt;
        flag.Parse()

&lt;span class=&#34;gi&#34;&gt;+       log.SetLevelFromString(*logLevel)
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       log.SetHandler(jsonlog.Default)
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;&lt;/span&gt;        return config{
                socketPath: *sockPath,
                nodeID:     *nodeID,
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And then thread that through the RPC servers.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-diff&#34; data-lang=&#34;diff&#34;&gt; type NodeServer struct {
&lt;span class=&#34;gd&#34;&gt;-       NodeID   string
&lt;/span&gt;&lt;span class=&#34;gd&#34;&gt;-       Topology map[string]string
&lt;/span&gt;&lt;span class=&#34;gd&#34;&gt;&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       nodeID   string
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       topology map[string]string
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       log      *log.Entry
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+}
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+func NewNodeServer(nodeID string, topology map[string]string) *NodeServer {
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       return &amp;amp;NodeServer{
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+               nodeID:   nodeID,
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+               topology: topology,
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+               log:      log.WithFields(log.Fields{&amp;#34;service&amp;#34;: &amp;#34;Node&amp;#34;}),
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       }
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;&lt;/span&gt; }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The resulting structured logs look like the following when I do &lt;code&gt;nomad alloc logs -stderr :alloc_id&lt;/code&gt;. I&#39;m not wild about the timing traces
being at &lt;code&gt;INFO&lt;/code&gt; but I can live with it. That&#39;s committed as
&lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/6b0de82c0dde3ded8b310b97b3bb5ca7267fc3b3&#34;&gt;6b0de82&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;{&amp;quot;fields&amp;quot;:{&amp;quot;duration&amp;quot;:0},&amp;quot;level&amp;quot;:&amp;quot;info&amp;quot;,&amp;quot;timestamp&amp;quot;:&amp;quot;2021-02-21T20:58:58.462860527Z&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;GetPluginCapabilities&amp;quot;}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Next time, I&#39;ll finally start making some ZFS datasets!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;If you find yourself saying this, stop and check what you&#39;re
doing. As we&#39;ll discover in a moment. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>A ZFS Driver for Nomad, Part 2</title>
      <link>https://blog.0x74696d.com/posts/zfs-driver-for-nomad-part2/</link>
      <pubDate>Mon, 15 Feb 2021 12:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/zfs-driver-for-nomad-part2/</guid>
      <description>&lt;p&gt;A keen observer will note the title of this series has been
altered. In the previous post I discussed an option to have a device
driver that communicated with the Nomad API. Because device plugins
are launched via
&lt;a href=&#34;https://github.com/hashicorp/go-plugin&#34;&gt;&lt;code&gt;go-plugin&lt;/code&gt;&lt;/a&gt;, we don&#39;t
typically have to worry about securing their communication with
Nomad. But if the plugin were to talk to the Nomad HTTP API, we&#39;d need
to give it ACL tokens and certificates for mTLS. This was going to
introduce a bunch of operational lifecycle complexity I don&#39;t want to
deal with.&lt;/p&gt;
&lt;p&gt;This design exercise has certainly given me some interesting things to
think about for the future of Nomad&#39;s Device Plugin API. But it looks
like the best way to move forward is to implement the ZFS plugin as a
CSI driver. &lt;em&gt;Sigh&lt;/em&gt;. Fine. Let&#39;s get to it.&lt;/p&gt;
&lt;p&gt;If I take a look at the &lt;a href=&#34;https://github.com/container-storage-interface&#34;&gt;CSI organization on
GitHub&lt;/a&gt;, there&#39;s a
notable lack of starter projects or examples. That&#39;s because the
developer community for CSI is working over in the &lt;a href=&#34;https://github.com/kubernetes-csi&#34;&gt;Kubernetes
CSI&lt;/a&gt; organization. There&#39;s not
exactly a skeleton project there either, but we do have the
&lt;a href=&#34;https://github.com/kubernetes-csi/drivers&#34;&gt;drivers&lt;/a&gt; repo which looks
promising at first. That includes a
&lt;a href=&#34;https://github.com/kubernetes-csi/drivers/tree/master/pkg/csi-common&#34;&gt;&lt;code&gt;csi-common&lt;/code&gt;&lt;/a&gt;
package, but that was last updated 2 years ago. Most of this code
seems to have made its way over to the
&lt;a href=&#34;https://github.com/kubernetes-csi/csi-driver-host-path&#34;&gt;&lt;code&gt;csi-driver-host-path&lt;/code&gt;&lt;/a&gt;
plugin repo, so that should serve as a good guide for the bits of the
spec that Kubernetes implements. There are also some reasonably solid
&lt;a href=&#34;https://kubernetes-csi.github.io/docs/developing.html&#34;&gt;developer
docs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I also did a quick survey of the landscape and found the
&lt;a href=&#34;https://github.com/democratic-csi/democratic-csi&#34;&gt;&lt;code&gt;democratic-csi&lt;/code&gt;&lt;/a&gt;
project, which aims to be a framework for CSI plugins. But those folks
are writing plugins in NodeJS, so that&#39;s not going to help me. But,
hey, it&#39;s cool that they at least acknowledge
&lt;a href=&#34;https://github.com/democratic-csi/democratic-csi/blob/master/docs/nomad.md&#34;&gt;Nomad&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Time to get to coding. I&#39;ll start by creating the repo in GitHub &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;,
including the MPL2 license and a golang &lt;code&gt;.gitignore&lt;/code&gt; file. I clone
that down and run &lt;code&gt;go mod init github.com/tgross/zfs-csi-driver&lt;/code&gt;. I
know we&#39;ll need the protobufs from the CSI spec library, and I know
from having worked on the orchestrator side that this is a gRPC
service, so I&#39;ll grab that while I&#39;m at it:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;require &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;
        github.com/container-storage-interface/spec v1.3.0
        google.golang.org/grpc v1.35.0
&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The CSI specification is broken into three gRPC services:
&lt;strong&gt;Identity&lt;/strong&gt;, &lt;strong&gt;Node&lt;/strong&gt;, and &lt;strong&gt;Controller&lt;/strong&gt;. I&#39;ll dig into the Node and
Controller services in later posts, but all plugins need to run the
Identity service. Nomad will call the Identity service when the plugin
task starts and for liveness checks, and the plugin responds with
metadata and capabilities.&lt;/p&gt;
&lt;p&gt;In CSI the plugin implements the &amp;quot;server&amp;quot; and the orchestrator (Nomad
or Kubernetes) is the client. So I&#39;ll start by implementing an empty
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/v1.3.0/lib/go/csi/csi.pb.go#L5185-L5190&#34;&gt;&lt;code&gt;IdentityServer&lt;/code&gt;
interface&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;kd&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;IdentityServer&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;struct&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{}&lt;/span&gt;

&lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;IdentityServer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;GetPluginInfo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
	&lt;span class=&#34;nx&#34;&gt;context&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Context&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;csipb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;GetPluginInfoRequest&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
	&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;csipb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;GetPluginInfoResponse&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;error&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
	&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;nil&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;nil&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;IdentityServer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;GetPluginCapabilities&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
	&lt;span class=&#34;nx&#34;&gt;context&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Context&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;csipb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;GetPluginCapabilitiesRequest&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
	&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;csipb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;GetPluginCapabilitiesResponse&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;error&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
	&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;nil&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;nil&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;IdentityServer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;Probe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;context&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Context&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;csipb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;ProbeRequest&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
	&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;csipb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;ProbeResponse&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;error&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
	&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;nil&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;nil&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I&#39;m just dropping all this in &lt;code&gt;main.go&lt;/code&gt; for the moment. As the series
goes on I&#39;ll factor each of the services out into their own files. For
now I just want to make sure I&#39;ve got all the dependencies figured
out. I&#39;ll instantiate a unix socket listener with some very crude
argument parsing, and wire that up to an out-of-the-box gRPC
server. I&#39;ll run that, just making sure it compiles and that it binds
to the socket file. That&#39;s &lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/31e0be537de241fa1e76c793d016cb4e5afe8d94&#34;&gt;31e0be5&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next I&#39;ll take a quick detour to add the binary output to our
gitignore, and whip up a makefile. The &lt;code&gt;build&lt;/code&gt; target is simple for
now but I always end up wanting to add a bunch of flags later. And I
add a &lt;code&gt;check&lt;/code&gt; target to run some linting and static analysis. That&#39;s
&lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/722669e67095f8b30bc28af55439a3602e56a048&#34;&gt;722669e&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make check
gofmt ......... ok!
go vet ........ ok!
staticcheck ... ok!
go mod tidy ... ok!
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With our &lt;em&gt;mise&lt;/em&gt; solidly &lt;em&gt;en place&lt;/em&gt;, I&#39;ll get the Identity service into
enough shape where I can at least make sure it&#39;ll register itself as a
CSI plugin. I&#39;ll have both the &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#identity-service-rpc&#34;&gt;Identity service
spec&lt;/a&gt;
and the generated &lt;a href=&#34;https://pkg.go.dev/github.com/container-storage-interface/spec@v1.3.0/lib/go/csi&#34;&gt;library
docs&lt;/a&gt;
handy.&lt;/p&gt;
&lt;p&gt;First I need to return a
&lt;a href=&#34;https://pkg.go.dev/github.com/container-storage-interface/spec@v1.3.0/lib/go/csi#GetPluginInfoResponse&#34;&gt;&lt;code&gt;GetPluginInfoResponse&lt;/code&gt;&lt;/a&gt;. I
don&#39;t think I have a use for the manifest field, but I&#39;ll leave that
commented out here to remind myself later.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-diff&#34; data-lang=&#34;diff&#34;&gt;&lt;span class=&#34;gi&#34;&gt;+const (
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       pluginName    = &amp;#34;zfs.csi.0x74696d.com&amp;#34;
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       pluginVersion = &amp;#34;0.0.1&amp;#34;
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+)
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;&lt;/span&gt; type IdentityServer struct{}

 func (i *IdentityServer) GetPluginInfo(context.Context, *csipb.GetPluginInfoRequest) (
        *csipb.GetPluginInfoResponse, error) {
&lt;span class=&#34;gd&#34;&gt;-       return nil, nil
&lt;/span&gt;&lt;span class=&#34;gd&#34;&gt;&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       return &amp;amp;csipb.GetPluginInfoResponse{
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+               Name:          pluginName,
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+               VendorVersion: pluginVersion,
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+               // Manifest: map[string]string{}, // TODO?
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       }, nil
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;&lt;/span&gt; }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next is the
&lt;a href=&#34;https://pkg.go.dev/github.com/container-storage-interface/spec@v1.3.0/lib/go/csi#GetPluginCapabilitiesResponse&#34;&gt;&lt;code&gt;GetPluginCapabilitiesResponse&lt;/code&gt;&lt;/a&gt;. I&#39;m
expecting that I&#39;ll want to implement the optional Controller service
and that I&#39;ll want to be able to tell Nomad not to provision onto
particular nodes. So I&#39;ll add both those capabilities to the
response. The constuctor for the &lt;code&gt;PluginCapability&lt;/code&gt; is pretty gross,
but I&#39;ve come to expect that from protobuf-generated code.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-diff&#34; data-lang=&#34;diff&#34;&gt; func (i IdentityServer) GetPluginCapabilities(
        context.Context, *csipb.GetPluginCapabilitiesRequest) (
        *csipb.GetPluginCapabilitiesResponse, error) {
&lt;span class=&#34;gd&#34;&gt;-       return nil, nil
&lt;/span&gt;&lt;span class=&#34;gd&#34;&gt;&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       return &amp;amp;csipb.GetPluginCapabilitiesResponse{
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+               Capabilities: []*csipb.PluginCapability{
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                       {
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                               Type: &amp;amp;csipb.PluginCapability_Service_{
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                                       Service: &amp;amp;csipb.PluginCapability_Service{
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                                               Type: csipb.PluginCapability_Service_CONTROLLER_SERVICE,
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                                       },
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                               },
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                       },
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                       {
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                               Type: &amp;amp;csipb.PluginCapability_Service_{
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                                       Service: &amp;amp;csipb.PluginCapability_Service{
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                                               Type: csipb.PluginCapability_Service_VOLUME_ACCESSIBILITY_CONSTRAINTS,
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                                       },
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                               },
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+                       },
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+               },
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       }, nil
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;&lt;/span&gt; }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And lastly is the
&lt;a href=&#34;https://pkg.go.dev/github.com/container-storage-interface/spec@v1.3.0/lib/go/csi#ProbeResponse&#34;&gt;&lt;code&gt;Probe&lt;/code&gt;&lt;/a&gt;
RPC, which has the dubious distinction of not returning a simple bool
for the &lt;code&gt;Ready&lt;/code&gt; field, but wrapping the type in some Google protobuf
library helper. So I had to add that to my imports.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-diff&#34; data-lang=&#34;diff&#34;&gt; func (i *IdentityServer) Probe(context.Context, *csipb.ProbeRequest) (
        *csipb.ProbeResponse, error) {
&lt;span class=&#34;gd&#34;&gt;-       return nil, nil
&lt;/span&gt;&lt;span class=&#34;gd&#34;&gt;&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       return &amp;amp;csipb.ProbeResponse{
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+               Ready: &amp;amp;pbwrappers.BoolValue{Value: true},
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+       }, nil
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;&lt;/span&gt; }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That&#39;s it for now with the Identity service
(&lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/27e646eccb4eb63cb3bb148b8db97c1a4fad7589&#34;&gt;27e646e&lt;/a&gt;). In
a later post I&#39;ll have these RPC endpoints fingerprint the plugin
environment to assert plugin health contingent on access to a
particular zpool, and that the plugin has whatever tools or libraries
it needs.&lt;/p&gt;
&lt;p&gt;Now to verify this runs on Nomad. Normally for development I&#39;d
probably prefer to run the plugin via the &lt;code&gt;raw_exec&lt;/code&gt; or &lt;code&gt;exec&lt;/code&gt; driver,
but a CSI plugin needs to be able to run with &lt;code&gt;CAP_SYSADMIN&lt;/code&gt; so unless
and until we give the &lt;code&gt;exec&lt;/code&gt; driver a much-needed refresh, I&#39;ll need
to use the &lt;code&gt;docker&lt;/code&gt; or &lt;code&gt;podman&lt;/code&gt; driver.&lt;/p&gt;
&lt;p&gt;So that I don&#39;t have to constantly rebuild the Docker image, I&#39;ll
bind-mount the binary into a standard container. My first pass at this
used the &lt;code&gt;busybox&lt;/code&gt; base image, but I was getting a perplexing &amp;quot;no such
file or directory&amp;quot;. After burning a few minutes debugging my Docker
mount configuration, I realized why:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ldd bin/zfs-csi-driver
        linux-vdso.so.1 (0x00007fff1fc47000)
        libpthread.so.0 =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f7e29b7e000)
        libc.so.6 =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f7e2978d000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f7e29d9d000)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;What gives? Doesn&#39;t go build statically-linked binaries? Yes, but if
you include any package that has C bindings, it&#39;ll be dynamically
linked by default. This includes &lt;code&gt;os/user&lt;/code&gt; and the all-important &lt;code&gt;net&lt;/code&gt;
package from the stdlib. So binding only the binary into the container
will only work if the container image includes these libraries. The
busybox base image is all statically linked, so there&#39;s no libc or
pthread to use. I could fix this with the &lt;code&gt;netgo&lt;/code&gt; build tag &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; or
&lt;code&gt;CGO_ENABLED=0&lt;/code&gt; but there&#39;s a pretty good chance I&#39;ll want to link to
libzfs later anyways. Instead I&#39;ll swap out for the &lt;code&gt;debian:buster&lt;/code&gt;
base image, which has the same libc and other libraries as my
development machine. That&#39;s
&lt;a href=&#34;https://github.com/tgross/zfs-csi-driver/commit/46daf091a20c2c0eedc2459f3058a579f2a3a48c&#34;&gt;46daf09&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I fire up a Nomad dev agent and run the plugin job:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nomad plugin status csi-zfs
ID                   = csi-zfs
Provider             = zfs.csi.0x74696d.com
Version              = 0.0.1
Controllers Healthy  = 0
Controllers Expected = 1
Nodes Healthy        = 0
Nodes Expected       = 1

Allocations
No allocations placed
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The plugin registers, but it&#39;s not being marked healthy. We can see
why in the Nomad client logs:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;2021-02-15T15:47:33.838-0500 [WARN] client.csi-zfs: finished client
unary call: grpc.code=Unimplemented duration=512.367s
grpc.service=csi.v1.Controller grpc.method=ControllerGetCapabilities&lt;/p&gt;
&lt;p&gt;2021-02-15T15:47:33.839-0500 [WARN] client.csi-zfs: finished client
unary call: grpc.code=Unimplemented duration=357.548s
grpc.service=csi.v1.Node grpc.method=NodeGetInfo&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I haven&#39;t implemented the Controller and Node services yet! Next time,
I&#39;ll talk a bit more about the architecture of a CSI plugin, what
these services do, and hopefully get the plugin into a healthy status.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Which I always forget proves to be annoying when they use the
default email address for the initial commit, which is currently
my work address. Although I suppose all this code is copyright my
employer anyways. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Passing &lt;code&gt;-tags netgo&lt;/code&gt; forces go to use a pure-go implementation
of &lt;code&gt;net&lt;/code&gt; that doesn&#39;t use &lt;code&gt;getaddrinfo&lt;/code&gt; or other libc functions,
but if you&#39;re implementing a client rather than a server I&#39;d
recommend against this because it introduces some operational
gotchas with DNS. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Small Design Up Front</title>
      <link>https://blog.0x74696d.com/posts/small-design-up-front/</link>
      <pubDate>Sat, 13 Feb 2021 12:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/small-design-up-front/</guid>
      <description>&lt;p&gt;At my current gig and several before that, the initial engineering
design document is the Request for Comments (RFC), sometimes called
the &lt;a href=&#34;https://github.com/joyent/rfd&#34;&gt;Request for Discussion&lt;/a&gt; (RFD).&lt;/p&gt;
&lt;p&gt;If you&#39;ve been reading the series on building a ZFS plugin for Nomad,
you might have asked yourself if this kind of stumbling through the
design is typical of the RFC documents I&#39;ve written. But that series
is really about all the design work that happens before the initial
design is documented. It&#39;s brainstorming, hypothesizing, and
exploration. The first draft of a RFC is the output of that work, so
typically by the time anyone else has seen it hopefully the obviously
dumb ideas and dead ends have been weeded out.&lt;/p&gt;
&lt;p&gt;The RFC ends up being a good &amp;quot;sandbox&amp;quot; for a small up-front design
process. I suspect it&#39;s especially valuable for system software where
even a minimal experiment can be costly. And the structure discourages
you from trying to come up with a rigid specification that&#39;s doomed to
be invalid the moment you start implementing it.&lt;/p&gt;
&lt;p&gt;In some sense you&#39;re writing a RFC to communicate to your peers what
you&#39;ve already figured out about the problem. Their time is a gift,
and the most valuable feedback to get is that which you couldn&#39;t think
of on your own. So you should invest the time to ensure they&#39;re not
just going to tell what you should already know. In a healthy
organization, writing is a way to collectively discover the design,
rather than persuade the team.&lt;/p&gt;
&lt;p&gt;The phrase &amp;quot;in a healthy organization&amp;quot; is doing a lot of work
here. I&#39;ve worked places where RFC discussions were more of a battle
ground for interpersonal conflict and office politics than meaningful
engineering discussion. In that environment you end up writing
defensively to head off debate and hide implementation details that
will trigger objections. These documents are better named Request for
Permission. And if you&#39;re in this situation... well, writing RFCs ain&#39;t
gonna save you.&lt;/p&gt;
&lt;p&gt;I&#39;m probably a weird outlier, but I even write RFCs for personal
projects. Call it writing as structured thought experiment. It&#39;s a
tool, and one that supplements rather than replaces a whiteboard
diagram or a throwaway spike. I could easily throw it out as soon as
it&#39;s done, but why not keep it?&lt;/p&gt;
&lt;p&gt;There can be a few audiences for those artifacts. A project might get
completed to the point where it could be open sourced, in which case
having those early design documents would be valuable to users or
contributors. But the most important audience is Future Me. My level
of interest or volume of work on these projects ebbs and flows. I
might take a project through the initial design, feel like I&#39;ve
explored the problem well enough to learn what I wanted to learn, and
set it aside for months. The RFC is like a well-written commit message
for the project as a whole.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A ZFS Device Driver for Nomad, Part 1</title>
      <link>https://blog.0x74696d.com/posts/zfs-device-driver-for-nomad-part1/</link>
      <pubDate>Sun, 07 Feb 2021 12:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/zfs-device-driver-for-nomad-part1/</guid>
      <description>&lt;p&gt;After yesterday&#39;s post, my colleague Chris Baker
&lt;a href=&#34;https://twitter.com/ScaredOfGeese/status/1358126170236203011&#34;&gt;Tweeted&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I&#39;m especially curious whether the device lifecycle is gonna give
you the hooks you need without hacks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ominous foreshadowing.&lt;/p&gt;
&lt;p&gt;The first challenge I identified in the device plugin API was that the
plugin gets a list of device IDs in the
&lt;a href=&#34;https://www.nomadproject.io/docs/internals/plugins/devices#reserve-deviceids-string-containerreservation-error&#34;&gt;&lt;code&gt;Reserve&lt;/code&gt;&lt;/a&gt;
method, but the scheduler only knows what device IDs are available
from the fingerprint. We can create the datasets ahead of time out of
band, but that&#39;s not a great experience for the job
submitter. Coincidentally, this is similar to what we face with
&lt;a href=&#34;https://www.nomadproject.io/docs/internals/plugins/csi&#34;&gt;CSI&lt;/a&gt; in
claiming a unique volume per allocation. That&#39;s going to require
scheduler changes, which I&#39;m working on in
&lt;a href=&#34;https://github.com/hashicorp/nomad/issues/7877#issuecomment-772552412&#34;&gt;nomad/#7877&lt;/a&gt;. So
maybe that implementation for volumes should take into account wanting
to apply &amp;quot;unique per allocation&amp;quot; interpolation to other resources.&lt;/p&gt;
&lt;p&gt;Assuming we somehow solve that problem, there&#39;s another that comes to
mind. I&#39;m looking at the API for the device plugin and I see &lt;code&gt;Reserve&lt;/code&gt;
without a mirrored method to release the claim when we&#39;re done with
it. And if we&#39;re reserving a device, shouldn&#39;t it be for a particular
allocation? Then I see this part of the
&lt;a href=&#34;https://www.nomadproject.io/docs/internals/plugins/devices#lifecycle-and-state&#34;&gt;docs&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;After helping to provision a task with a scheduled device, a device
plugin does not have any responsibility (or ability) to monitor the
task.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hold up. This is reminding me of the old Seinfeld bit about how anyone
can just take reservations. How does this actually work?&lt;/p&gt;
&lt;p&gt;Now at this point you might be asking yourself how an experienced
engineer who&#39;s been working on a code base all day every day for a
year and a half can simply &lt;em&gt;not know&lt;/em&gt; how a feature like this
works. Some of that is size: Nomad is 400k lines of go code, plus
another 50k or so of JavaScript, not counting vendored
dependencies. Roughly half of that is tests. That&#39;s a lot to digest.&lt;/p&gt;
&lt;p&gt;But Nomad is also reasonably well-architected (with plenty of room for
improvement, of course!). Many features can be implemented as discrete
&amp;quot;hooks&amp;quot; that get called by the various event loops. So once you get
one of those features working you can mentally unload that context and
it will need minimal maintenance. Abstraction of components isn&#39;t good
just for the sake of it, but because it lets mere mortal developers
like me build software solving galaxy brain problems.&lt;/p&gt;
&lt;p&gt;With that out of the way, let&#39;s look at what&#39;s happening under the
hood with the device plugin API. Note that throughout this section I&#39;m
linking to a specific tag so that the line references don&#39;t change
over time. If you&#39;re reading this much later, you may find you want to
look for the same functions at different line numbers on the current
version.&lt;/p&gt;
&lt;p&gt;Each Nomad client has a device manager that runs the device
plugins. Each instance it tracks runs a fingerprint: one at start and
then periodically. The &lt;a href=&#34;https://github.com/hashicorp/nomad/blob/v1.0.3/client/devicemanager/instance.go#L338&#34;&gt;instance
fingerprint&lt;/a&gt;
asks the plugin for a
&lt;a href=&#34;https://github.com/hashicorp/nomad/blob/v1.0.3/plugins/device/device.go#L40-L48&#34;&gt;&lt;code&gt;FingerprintResponse&lt;/code&gt;&lt;/a&gt;.
We get back a list of devices, which according to the docs we&#39;re
supposed to assume are interchangeable from the perspective of the
scheduler.&lt;/p&gt;
&lt;p&gt;The fingerprint goes up to the device manager, which calls its
&lt;a href=&#34;https://github.com/hashicorp/nomad/blob/v1.0.3/client/devicemanager/manager.go#L191&#34;&gt;&lt;code&gt;updater&lt;/code&gt;&lt;/a&gt;
to add the fingerprint to the client node&#39;s state. This makes its way
via the
&lt;a href=&#34;https://github.com/hashicorp/nomad/blob/v1.0.3/nomad/node_endpoint.go#L375&#34;&gt;&lt;code&gt;Node.UpdateStatus&lt;/code&gt;&lt;/a&gt;
RPC to the server, where it finally gets persisted as a
&lt;a href=&#34;https://github.com/hashicorp/nomad/blob/v1.0.3/nomad/structs/structs.go#L3066-L3074&#34;&gt;&lt;code&gt;NodeDeviceResource&lt;/code&gt;&lt;/a&gt;
in the state store along with the rest of the client&#39;s resources. At
this point, the servers know what devices are available on the
client. This is the root of our problem around unique device IDs; the
plugin is telling the server what device IDs are available, and not
the other way around.&lt;/p&gt;
&lt;p&gt;Now let&#39;s see what happens when we try to schedule a job with one of
these devices. I&#39;m going to skip past most of the scheduler logic here
but check out my colleague
&lt;a href=&#34;https://github.com/schmichael&#34;&gt;schmichael&#39;s&lt;/a&gt; awesome &lt;a href=&#34;https://www.youtube.com/watch?v=m6DnmVqoXvw&#34;&gt;deep
dive&lt;/a&gt; if you want to
learn more. tl;dr we eventually get to a point where the scheduler has
to rank which nodes can best fulfill the request. In the ranking
iterator we attempt to get an
&lt;a href=&#34;https://github.com/hashicorp/nomad/blob/v1.0.3/scheduler/rank.go#L357-L360&#34;&gt;&amp;quot;offer&amp;quot;&lt;/a&gt;
for that device. The
&lt;a href=&#34;https://github.com/hashicorp/nomad/blob/v1.0.3/scheduler/device.go#L29-L32&#34;&gt;&lt;code&gt;AssignDevice&lt;/code&gt;&lt;/a&gt;
method checks the placement&#39;s
&lt;a href=&#34;https://github.com/hashicorp/nomad/blob/v1.0.3/scheduler/feasible.go#L1264-L1288&#34;&gt;feasibility&lt;/a&gt;,
or whether the node has enough of the requested devices available that
match our constraints. But note that the scheduler is checking that
the &lt;em&gt;server&#39;s&lt;/em&gt; state of the world says that we have enough of the
devices, and not communicating with the plugin at this point. Nomad&#39;s
scheduler workers always work with an in-memory snapshot of the server
state and don&#39;t perform I/O until they submit the plan to the leader.&lt;/p&gt;
&lt;p&gt;So when do we talk to the device plugin? Once the plan is made and the
client receives a placement for the allocation, the client fires a
series of hooks for the allocation and all the tasks in the
allocation. The &lt;a href=&#34;https://github.com/hashicorp/nomad/blob/v1.0.3/client/allocrunner/taskrunner/device_hook.go&#34;&gt;device pre-start
hook&lt;/a&gt;
is what finally takes the list of device IDs and calls the plugin&#39;s
&lt;a href=&#34;https://github.com/hashicorp/nomad/blob/v1.0.3/client/allocrunner/taskrunner/device_hook.go#L48-L49&#34;&gt;&lt;code&gt;Reserve&lt;/code&gt;&lt;/a&gt;
method.&lt;/p&gt;
&lt;p&gt;But just as we suspected from the plugin API, there&#39;s no matching
post-stop hook. The Nomad server is responsible for keeping track of
whether or not a device has been reserved. Which makes &lt;code&gt;Reserve&lt;/code&gt; a bit
of a misnomer. Nomad is not expecting the device plugin to reserve the
device, but it&#39;s telling the device plugin that Nomad &lt;em&gt;has reserved&lt;/em&gt;
the device, and to tell the client where it&#39;s been mounted.&lt;/p&gt;
&lt;p&gt;Which leaves us in a tricky spot.&lt;/p&gt;
&lt;p&gt;The device plugin can only get the state of the device via the
fingerprint, so unless there&#39;s a visible side-effect of the device
being used by a task, the plugin doesn&#39;t know when a task is done with
the device.&lt;/p&gt;
&lt;p&gt;Well, I did warn you that this series would include mistakes and dead
ends. Unfortunately it looks like we&#39;re beyond the point of &amp;quot;without
hacks&amp;quot;, so what are my options?&lt;/p&gt;
&lt;p&gt;First, I could certainly &amp;quot;cheat&amp;quot; and try to get a change in the device
plugin behavior into Nomad itself. Perhaps the plugin should be sent a
notice that the device isn&#39;t needed? Or maybe the &lt;code&gt;Reserve&lt;/code&gt; API should
have more information about the allocation? But that&#39;s the riskiest
approach because we&#39;re pretty serious about backwards compatibility in
Nomad&#39;s APIs, so we&#39;d have to live with that change for a long
time. And besides, our team has plenty of more important work on their
plate than my silly experiments!&lt;/p&gt;
&lt;p&gt;I could implement this whole workflow as a separate pre-start task
using a
&lt;a href=&#34;https://www.nomadproject.io/docs/job-specification/lifecycle&#34;&gt;&lt;code&gt;lifecycle&lt;/code&gt;&lt;/a&gt;
block (this was suggested by
&lt;a href=&#34;https://twitter.com/anapsix/status/1358339913868079105&#34;&gt;@anapsix&lt;/a&gt; on
Twitter). We often recommend pre-start tasks because they make a great
&amp;quot;escape hatch&amp;quot; for Nomad feature requests that we&#39;re unsure if we want
to implement, or ones we just don&#39;t have the time for on the road
map. But in this case it doesn&#39;t meet the design requirement of having
a separate operator and job submitter persona. The pre-start task
would have to be privileged and that lets a job submitter execute
arbitrary code as root on the host.&lt;/p&gt;
&lt;p&gt;A variant on that idea would be to implement a custom task driver that
&lt;em&gt;only&lt;/em&gt; exposes a dataset for other tasks in the allocation. This would
improve on the arbitrary pre-start task by having a &lt;a href=&#34;https://www.nomadproject.io/docs/configuration/plugin&#34;&gt;plugin
configuration&lt;/a&gt;
controlled by the operator. There are a few disadvantages to a task
driver: there would be extra running processes for each dataset, and
implementing a task driver is just a lot more work to implement. But a
task driver already has hooks for the task&#39;s lifecycle, so it would
let Nomad manage when the ZFS workflows happen.&lt;/p&gt;
&lt;p&gt;Or I could implement a CSI plugin after all. I had wanted to steer
away from that because of how painful I&#39;d found CSI plugins. But at
least I know in this case the CSI plugin will be well-behaved with
Nomad. Curses.&lt;/p&gt;
&lt;p&gt;Lastly, just because the Nomad plugin API doesn&#39;t do what I want,
doesn&#39;t mean my device plugin client couldn&#39;t also communicate with
Nomad via its HTTP API. The plugin is already privileged code running
as Nomad&#39;s user (typically root), so it could make blocking queries to
the client to get allocation state on its own node.&lt;/p&gt;
&lt;p&gt;Although weeks of coding can save hours of planning, I think I&#39;m at
the point where it&#39;s time to do some small experiments to explore
these options. But next post I want to take a short detour to talk
about RFCs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A ZFS Device Driver for Nomad, Part 0</title>
      <link>https://blog.0x74696d.com/posts/zfs-device-driver-for-nomad-part0/</link>
      <pubDate>Sat, 06 Feb 2021 12:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/zfs-device-driver-for-nomad-part0/</guid>
      <description>&lt;p&gt;I&#39;ve been the primary maintainer of Nomad&#39;s various storage
implementations since this past summer when the last other person
who&#39;d worked on it left HashiCorp. That includes host volumes and the
Container Storage Interface (CSI), both of which were originally
designed by someone smarter than me.&lt;/p&gt;
&lt;p&gt;But CSI is a bit of a mess in terms of providing any actual
abstraction. The spec focuses almost entirely on the protobuf
plumbing, several plugins don&#39;t comply with the specified concurrency
requirements or otherwise have Kubernetes-specific quirks, and half
the implementation in Kubernetes is out-of-tree or simply
unimplemented. That leads to things like plugins having implementation
bug in interfaces that no one has ever tested until Nomad came along
and tried it, and the plugin authors don&#39;t really care to fix it.&lt;/p&gt;
&lt;p&gt;Arguably the notion of having a storage abstraction that abstracts
both the storage provider and the container orchestrator is ill
conceived from the get-go. Operators aren&#39;t running multiple
orchestrators and dozens of storage implementations; they&#39;re typically
going to be responsible for one orchestrator and at most two or three
semantically-incompatible storage providers (ex. a block storage, an
object storage, and maybe a network file system). The interfaces are
rightfully the storage vendor&#39;s or orchestrator developer&#39;s problem,
not yours as an end user. Instead we&#39;ve dropped a steaming pile of
operational gotchas on your plate. So CSI is the sort of thing that
you&#39;d try to make a standard if you were a giant tech company with a
history of pushing half-baked standards on the rest of the industry
for purposes of cementing your market dominance. Hypothetically
speaking, of course.&lt;/p&gt;
&lt;p&gt;All that to say when I wanted to have a way to provide ZFS datasets to
a Nomad workload, I definitely didn&#39;t want to implement a CSI storage
provider for it. I have to deal with CSI during my day job. Yes, Nomad
is also my day job. Point taken.&lt;/p&gt;
&lt;p&gt;Most of my little experiments get worked on in private, but the scope
of this is small and relevant to my employment so I thought I&#39;d share
my process as I go. With only an hour or two here and there to spend,
I&#39;m still hoping that this will take no more than a couple
weeks. Despite having been on the Nomad team for a while now, I
haven&#39;t had to dig into the device plugin system much. So expect
plenty of exploration, dead ends, rework, and plain old mistakes.&lt;/p&gt;
&lt;p&gt;Let&#39;s start with a design. At HashiCorp or Joyent this would be an RFC
(Request For Comments) doc. While I don&#39;t need that level of formality
here, it&#39;s worth having the outline of a story for what I&#39;m trying to
do up front.&lt;/p&gt;
&lt;p&gt;Nomad has two primary user personas: the cluster operator and the job
submitter. In small orgs these will be the same people, but typically
the cluster operators are the folks responsible for the underlying
platform and have root on the Nomad hosts, whereas the job submitter
will have the rights to submit jobs to Nomad and varied levels of
access to debug them when things go wrong.&lt;/p&gt;
&lt;p&gt;When I submit a job, I want to expose a ZFS dataset to my workload. I
want that ZFS dataset to be backed up. And when my workload gets
rescheduled I want the ZFS dataset to be available wherever the
workload is placed. Immediately this brings to mind ZFS snapshots and
ZFS send / receive. The existing
&lt;a href=&#34;https://www.nomadproject.io/docs/job-specification/migrate&#34;&gt;&lt;code&gt;migrate&lt;/code&gt;&lt;/a&gt;
feature in Nomad doesn&#39;t have any hooks for plugins, so I&#39;ll probably
have to implement much of the ZFS send and receive workflow from
scratch.&lt;/p&gt;
&lt;p&gt;Putting on my operator hat, I want all datasets a job submitter can
create to be children of a particular dataset given to Nomad. Of
course I&#39;ll also want disk quotas.&lt;/p&gt;
&lt;p&gt;The best fit for these requirements is a &lt;a href=&#34;https://www.nomadproject.io/docs/internals/plugins/devices&#34;&gt;device
plugin&lt;/a&gt;,
as I want to be able to mount data to arbitrary workloads, and not
create a new workload type as I would with a driver plugin. Not many
device plugins have been written; there&#39;s the baked-in Nvidia GPU
plugin, a fun hack for an &lt;a href=&#34;https://github.com/cgbaker/nomad-device-raspberry-epaper-hat&#34;&gt;ePaper device
plugin&lt;/a&gt;
by my colleague &lt;a href=&#34;https://github.com/cgbaker&#34;&gt;Chris Baker&lt;/a&gt;, and a &lt;a href=&#34;https://gitlab.com/CarbonCollins/nomad-usb-device-plugin&#34;&gt;USB
storage
plugin&lt;/a&gt;
developed by Steven Collins out in the Nomad community. That&#39;s about
it. So I&#39;m going to assume I&#39;ll hit some sharp edges to workaround or
that might need patches in Nomad proper if they&#39;re useful for other
authors.&lt;/p&gt;
&lt;p&gt;The plugin needs to talk to ZFS. The hacky way to do this would be to
shell-out to the ZFS command line utilities. One advantage of this
would be that I can use the existence of the command line utilities as
part of our fingerprint, and I know those utilities will be compatible
whatever ZFS is on the host. That also makes packaging nicer. On the
other hand, it&#39;s awfully brittle to parse the command line output for
tools the plugin doesn&#39;t own, especially in the case of Nomad which is
running as a service and not interactively. So that&#39;s a detail to
figure out.&lt;/p&gt;
&lt;p&gt;The Nomad device plugin API I need to implement has just three
methods: &lt;code&gt;Fingerprint&lt;/code&gt;, &lt;code&gt;Stats&lt;/code&gt;, and &lt;code&gt;Reserve&lt;/code&gt;. Nomad will fingerprint
the device plugin to ask for the set of &lt;a href=&#34;https://pkg.go.dev/github.com/hashicorp/nomad@v1.0.3/plugins/device#FingerprintResponse&#34;&gt;available
devices&lt;/a&gt;. This
happens on client startup and then again periodically. The device
plugin is expected to provide
&lt;a href=&#34;https://pkg.go.dev/github.com/hashicorp/nomad@v1.0.3/plugins/device#DeviceStats&#34;&gt;stats&lt;/a&gt;
for the devices it manages. And Nomad wants to send a list of device
IDs to reserve, expecting the plugin to return a
&lt;a href=&#34;https://pkg.go.dev/github.com/hashicorp/nomad@v1.0.3/plugins/device#ContainerReservation&#34;&gt;reservation&lt;/a&gt;
object with the set of devices and mounts. Immediately I see our first
challenge: the API expects a known set of device IDs, whereas our
design wants the job submitter to be able to define datasets on the
fly. Is this project dead on arrival? That would be embarrassing!&lt;/p&gt;
&lt;p&gt;Next time, I&#39;ll look at the Nomad scheduler and see if I can come up
with a reasonable approach to handling the device ID.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Single purpose visualization</title>
      <link>https://blog.0x74696d.com/posts/single-purpose-visualization/</link>
      <pubDate>Sat, 30 Jan 2021 12:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/single-purpose-visualization/</guid>
      <description>&lt;p&gt;There&#39;s an enormous wealth of data analysis and visualization tools
available, from full-fledged managed services like Honeycomb all the
way down to Python libraries like Matplotlib. These days I&#39;m writing
shrink-wrapped infrastructure software, so when I&#39;m debugging problems
I&#39;ve been leaning way more to one end of that spectrum: writing single
purpose tools.&lt;/p&gt;
&lt;p&gt;I can just barely fool myself into thinking there&#39;s a Unix philosophy
at work here about tools that do one thing and do it well. But if I&#39;m
being honest most of these tools turn out to be throwaway because
they&#39;re operating at the wrong level of abstraction. I&#39;m solving the
problem immediately in front of me and not worrying about software
architecture unless and until I need it again. Also we&#39;re
overwhelmingly a golang shop, so trying to package up Python tools so
that support folks can reuse them is a burden.&lt;/p&gt;
&lt;p&gt;A recent example of this a tool I wrote for visualizing metrics from a
Nomad debug bundle. Nomad&#39;s (&lt;a href=&#34;https://www.nomadproject.io/docs/commands/operator/debug&#34;&gt;&lt;code&gt;operator debug&lt;/code&gt;&lt;/a&gt;)
gathers up a bunch of logs from the cluster and takes a series of
snapshots of the raft state and cluster metrics, and then dump this
whole thing into a tarball. Our support folks can use this for
gathering a ton of data about a customer problem without having to do
a long back-and-forth of questions, and when they need to escalate to
engineers they can hand off the bundle and we can make a first pass at
the problem without bothering the customer some more.&lt;/p&gt;
&lt;p&gt;The challenge is that this data is basically just a bunch of Nomad&#39;s
API responses or internal structs dumped out to JSON. A bundle for a
complex problem with lots of snapshots can easily be 100MB of JSON to
grub through.&lt;/p&gt;
&lt;p&gt;So suppose I want to find how many goroutines are running over time. I
look up the field name in the &lt;a href=&#34;https://www.nomadproject.io/docs/operations/metrics&#34;&gt;metrics
docs&lt;/a&gt;, check the
&lt;a href=&#34;https://github.com/hashicorp/nomad/blob/v1.0.3/api/operator_metrics.go#L8-L15&#34;&gt;&lt;code&gt;api.MetricsSummary&lt;/code&gt;&lt;/a&gt;
output for which fields that&#39;s going to be under, and I incrementally
massage my way through the JSON with trial and error and &lt;code&gt;jq&lt;/code&gt; until I
get something like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;ls nomad/*/metrics.json &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;
    xargs jq &lt;span class=&#34;s1&#34;&gt;&amp;#39;
&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;        .Gauges[]
&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;        | select(.Name == &amp;#34;nomad.runtime.num_goroutines&amp;#34;)
&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;        | .Value
&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;        &amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Yes, yes. I know I can use &lt;code&gt;find -exec&lt;/code&gt; instead. Take this and your
&amp;quot;useless use of cat&amp;quot; and leave me alone.&lt;/p&gt;
&lt;p&gt;The result is a list of numbers, and if I could understand the
&lt;code&gt;gnuplot&lt;/code&gt; interface I&#39;d probably pipe those numbers there. But
extracting timestamps from this data structure is really painful in
&lt;code&gt;jq&lt;/code&gt; and I&#39;ll never remember how to do it next time unless I save it
in a script somewhere, etc.&lt;/p&gt;
&lt;p&gt;This time I wanted to be able to show this to our support folks, so I
decided to turn it into a single purpose visualization tool that I
knew they could build. I grabbed the &lt;code&gt;gonum/plot&lt;/code&gt; library, which is
definitely not nearly as nice as Matplotlib but it got the job
done. The resulting
&lt;a href=&#34;https://github.com/tgross/nomad-metrics-plot&#34;&gt;&lt;code&gt;nomad-metrics-plot&lt;/code&gt;&lt;/a&gt;
tool takes a list of metrics files and generates a simple SVG (which
is pronounced &amp;quot;svig&amp;quot;, by the way) for one metric.&lt;/p&gt;
&lt;p&gt;If I want to see the latency between the raft leader and its peers, I
can pipe in the list of metrics files and search for that specific
metric:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;ls nomad/*/metrics.json &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;
    nomad-metrics-plot &lt;span class=&#34;s2&#34;&gt;&amp;#34;nomad.raft.leader.lastContact&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And the resulting visualization makes it obvious to me that this
cluster is having latency issues between raft peers: the mean and
maximum latency is well above what&#39;s recommended and they have spikes
where the 500ms timeout is being hit, which forces a leader election.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.0x74696d.com/images/20210130/metrics.svg&#34; alt=&#34;plot of raft.leader.lastContact metrics&#34;&gt;&lt;/p&gt;
&lt;p&gt;Note that the tool is terrible in many ways: the metric name has to be
an exact match, it has to read in the entire data set every time it
runs, there&#39;s no flag on where to send the output file, and it doesn&#39;t
open the SVG in your browser for you. But I can put this in front of
someone &lt;em&gt;today&lt;/em&gt; without it causing me a huge support burden to get
them spun up on it. And then I can iterate on it over time or abandon
it if something better comes along.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hugo 1-byte outputs</title>
      <link>https://blog.0x74696d.com/posts/hugo-one-byte-outputs/</link>
      <pubDate>Wed, 27 Jan 2021 12:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/hugo-one-byte-outputs/</guid>
      <description>&lt;p&gt;A particularly annoying bug in Hugo that I&#39;ve been running into is
that it will output 1-byte files for the index and other pages. This
is not sparking joy.&lt;/p&gt;
&lt;p&gt;What&#39;s particularly bad about it is that it&#39;s not at all
consistent. Any given build will just randomly decide that the front
page or the RSS feed should be empty. So every time I pushed to
Netlify it&#39;d be a crap shoot as to whether or not the whole site would
break. While I try to figure this out I&#39;ve been building locally,
checking manually with &lt;code&gt;find -size 1c&lt;/code&gt;, and then pushing the whole
build output directory to Netlify.&lt;/p&gt;
&lt;p&gt;This is a good example of where a tool has made so much of their
stated value proposition about performance that they seem to have
forgotten to do the job correctly. It&#39;s the MongoDB of static website
generator software. And it&#39;s totally undebuggable of course; they give
you no tools except for ones that help you debug rendering performance
(which, I am forced to admit, are &lt;a href=&#34;https://github.com/devopsdays/devopsdays-theme/issues/643&#34;&gt;pretty
nice&lt;/a&gt;). I
probably have some small template bug that&#39;s only triggering some
interleaved concurrent rendering path in Hugo when the moon is waxing
full, resulting in a file that contains only a single newline.&lt;/p&gt;
&lt;p&gt;Of course I dug through their GitHub issues looking for anything
similar and their answers always start with asking you to upgrade to
the very latest version. Which would be fine except that every single
time I&#39;ve updated Hugo they&#39;ve broken backwards compatibility in their
templates. If I wanted that kind of pain I would just fix the damn bug
myself. And hey it&#39;s open source so isn&#39;t that the beauty of it? But
it&#39;s a static website renderer with acute featuritis, which is exactly
the sort of nerd snipe that&#39;s going to find me writing my own from
scratch. As one does.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Main Branch</title>
      <link>https://blog.0x74696d.com/posts/main-branch/</link>
      <pubDate>Tue, 26 Jan 2021 12:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/main-branch/</guid>
      <description>&lt;p&gt;A short and mostly unserious rant.&lt;/p&gt;
&lt;p&gt;Much of the industry seems to have come around to changing the default
branch for git from &lt;code&gt;master&lt;/code&gt; to &lt;code&gt;main&lt;/code&gt;. It&#39;s absolutely terrible. Oh,
not changing from &lt;code&gt;master&lt;/code&gt;. That&#39;s fine. A modest improvement in
making our industry kinder. I&#39;m all for it. But we picked &lt;code&gt;main&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Literally any time anything to do with git comes up, you&#39;ve got a
whole mess of people complaining about what a terrible user experience
it has, and how hard it is to bring new people into the industry when
we use such unfriendly tools, and yada yada yada. And they&#39;re mostly
right.&lt;/p&gt;
&lt;p&gt;The git data model is awesome, but the command line interface is an
inconsistent disaster that we&#39;re all begrudgingly forced to learn. I
feel like I have a really solid understanding of the data model and
I&#39;m a rebasing and reflogging fiend, but I still end up having to
double-check the man page every time I get away from the twenty or so
commands I use on a regular basis. But we all recognize how hard it is
to change software that&#39;s been in widespread use. Backwards
compatibility is important.&lt;/p&gt;
&lt;p&gt;So there we were as a whole software industry, faced with a rare
opportunity to break free from a legacy decision...&lt;/p&gt;
&lt;p&gt;And we picked &lt;code&gt;main&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Main? &lt;em&gt;Main!?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The correct answer was &lt;code&gt;trunk&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Obviously.&lt;/p&gt;
&lt;p&gt;Choosing &lt;code&gt;main&lt;/code&gt; is exactly the sort of short-term local-maxima
thinking I&#39;ve come to expect from the industry. It&#39;s short, and it
preserves some muscle memory from &lt;code&gt;master&lt;/code&gt;. So you&#39;re saving typing
one (1) character over &lt;code&gt;trunk&lt;/code&gt;, assuming you&#39;re not a professional
with shell completion. And you&#39;re preserving muscle memory, which only
makes a difference during the month or so after your team has switched
away from &lt;code&gt;master&lt;/code&gt;, and only for the &lt;em&gt;set of people who are currently
using git&lt;/em&gt;. It means nothing to the umpteen million people who will be
coming into the industry over the next several decades (at least!)
that we&#39;ll be using git.&lt;/p&gt;
&lt;p&gt;Instead, we could have chosen &lt;code&gt;trunk&lt;/code&gt; and made some tiny marginal
improvement in the beginner&#39;s mental model for all those people. And
calling the default branch &lt;code&gt;trunk&lt;/code&gt; is &lt;em&gt;fun&lt;/em&gt;. Have some fun for fuck&#39;s
sake. A &lt;code&gt;main&lt;/code&gt; branch is dry toast.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exec from Your start.sh</title>
      <link>https://blog.0x74696d.com/posts/exec-from-your-start-script/</link>
      <pubDate>Mon, 25 Jan 2021 12:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/exec-from-your-start-script/</guid>
      <description>&lt;p&gt;At some point I noticed that some large portion of containers I&#39;ve
seen have some kind of &lt;code&gt;start.sh&lt;/code&gt; script file doing some setup and
then calling the actual application. Unfortunately a ton of these
break features of the application server. Like the previous post on
dropping signals, the way this typically manifests is the application
server can&#39;t reload configuration or gracefully shut down.&lt;/p&gt;
&lt;p&gt;Your standard user-friendly web frameworks run your code inside an
application server. This is what opens up a port, accepts connections,
and turns the data that comes into over those connections into some
kind of &amp;quot;request object&amp;quot;. Usually this will be a library separate from
your framework with some of its guts written in C, and the framework
will support a few options for servers. For Django this might be uwsgi
or gunicorn, for Rails it might be puma or unicorn, and for Spring it
might be Tomcat or Jetty.&lt;/p&gt;
&lt;p&gt;Most of these application servers have a bunch of nice features that
rely on signals, to reload the configuration, do graceful shutdown,
add extra worker processes, or whatever. And then some unlucky
developer gets handed a Dockerfile and gets told they have to use
that. But they have to load some config or do some setup at start
up. They look up how to do it and systemd has some &lt;code&gt;ExecStartPre&lt;/code&gt;
thing but this container stuff doesn&#39;t. And no one has ever bothered
to teach them what this is supposed to look like because developers
are only supposed to care about business logic anyways. So we end up
with a process tree in the container like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ps f -o pid,comm
  PID COMMAND
    1 /bin/sh start.sh
    8  \_ /usr/local/bin/gunicorn
   21     \_ gunicorn worker
   22     \_ gunicorn worker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now the orchestrator wants to tell the application to reload its
config and it sends a &lt;code&gt;SIGHUP&lt;/code&gt; to the container. By which we mean
PID1 in the container, which is our &lt;code&gt;start.sh&lt;/code&gt; script. It doesn&#39;t know
anything about signals, so it dies and takes the application with it.&lt;/p&gt;
&lt;p&gt;If you&#39;re using Docker you might have a
&lt;a href=&#34;https://github.com/krallin/tini&#34;&gt;&lt;code&gt;tini&lt;/code&gt;&lt;/a&gt; init process in there as
PID1 that&#39;ll pass signals to the &lt;code&gt;start.sh&lt;/code&gt; script, but the result is
the same because the signals never reach the application server.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ps f -o pid,comm
  PID COMMAND
    1 init
    7 \_ /bin/sh start.sh
   21     \_ /usr/local/bin/gunicorn
   22        \_ gunicorn worker
   23        \_ gunicorn worker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;What we wanted to do is to call &lt;code&gt;exec&lt;/code&gt; in our &lt;code&gt;start.sh&lt;/code&gt; script. Then
the process tree looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ps f -o pid,comm
  PID COMMAND
    1 /usr/local/bin/gunicorn
    7 \_ gunicorn worker
    8 \_ gunicorn worker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Doing this also means we can just set export environment variables in
the shell script and they&#39;ll be set in our new application server
process. A minimal working example looks like the following.&lt;/p&gt;
&lt;p&gt;Here&#39;s our mock application:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;make&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;chan&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Signal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;signal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Notify&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;syscall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;SIGINT&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;env&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;range&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Environ&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;fmt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Println&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;env&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
    &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;c&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;fmt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Println&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;\ngraceful shutdown!&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Our minimal startup script:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/sh
export SUPER_SECRET_FROM_VAULT=xyzzy
export PLATFORM=$(uname)
exec printenvvars
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And our Dockerfile:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FROM busybox:1
COPY printenvvars /bin/printenvvars
COPY start.sh /bin/start.sh
ENTRYPOINT [&amp;quot;/bin/start.sh&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We build that with &lt;code&gt;docker build -t test .&lt;/code&gt; and now let&#39;s run it:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker run test
HOSTNAME=741960aa5144
HOME=/root
SUPER_SECRET_FROM_VAULT=xyzzy
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PLATFORM=Linux
PWD=/
^C
graceful shutdown!
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note this will hang until we hit Ctrl-C, which sends &lt;code&gt;SIGINT&lt;/code&gt; to PID1
in the container. At that point the channel in our application
unblocks and we see the graceful shutdown message.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dropped Signals</title>
      <link>https://blog.0x74696d.com/posts/dropped-signals/</link>
      <pubDate>Sun, 24 Jan 2021 12:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/dropped-signals/</guid>
      <description>&lt;p&gt;A lot of go applications try to do something clever with signals and
end up dropping signals on the floor. I&#39;ve definitely written this
kind of bug myself. It&#39;s not a community practice to lean on an
application server rather than the stdlib, so that creates an
opportunity for folks to incorrectly implement it from scratch.&lt;/p&gt;
&lt;p&gt;Note that we&#39;re not talking about
&lt;a href=&#34;https://man7.org/linux/man-pages/man7/signal-safety.7.html&#34;&gt;&lt;code&gt;signal-safety(7)&lt;/code&gt;&lt;/a&gt;. For
purposes of this discussion we&#39;re going to merrily assume the authors
of &lt;a href=&#34;https://golang.org/pkg/os/signal/#Notify&#34;&gt;&lt;code&gt;os/signal.Notify&lt;/code&gt;&lt;/a&gt; have
avoided any signal-unsafe code. Although it&#39;d be neat to dig into how
that worked out with the go scheduler at some point.&lt;/p&gt;
&lt;p&gt;The docs for &lt;code&gt;os/signal.Notify&lt;/code&gt; say:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Package signal will not block sending to c: the caller must ensure
that c has sufficient buffer space to keep up with the expected
signal rate. For a channel used for notification of just one signal
value, a buffer of size 1 is sufficient.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We have to read this a bit carefully; it says a buffer of size 1 is
sufficient for one signal &lt;em&gt;value&lt;/em&gt;, which is not the same as one signal
type.&lt;/p&gt;
&lt;p&gt;Suppose we have a server that can reload its configuration on &lt;code&gt;SIGHUP&lt;/code&gt;
and does a graceful shutdown on &lt;code&gt;SIGINT&lt;/code&gt; (or &lt;code&gt;SIGTERM&lt;/code&gt;). If we&#39;re in
the middle of doing a configuration load and get a shutdown notice,
we&#39;ll queue-up the shutdown signal and process it afterwards. The
signal mask is still in place, so any other signal sent during that
window will get dropped.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;make&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;chan&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Signal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;signal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Notify&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;syscall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;SIGINT&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;syscall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;SIGHUP&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;nx&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;c&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;switch&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;case&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;syscall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;SIGHUP&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;nx&#34;&gt;fmt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Println&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Got SIGHUP, reloading config...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;nx&#34;&gt;time&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Sleep&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;time&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Second&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;case&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;syscall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;SIGINT&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;nx&#34;&gt;fmt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Println&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Got SIGINT, gracefully shutting down...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;nx&#34;&gt;time&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Sleep&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;time&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Second&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If we run this program in one terminal and then send it 3 signals in a
row, we can see we drop one of them.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# first terminal&lt;/span&gt;
$ go run .
Got SIGHUP, reloading config... hangup
Got SIGHUP, reloading config... hangup

&lt;span class=&#34;c1&#34;&gt;# second terminal&lt;/span&gt;
$ pkill -SIGHUP signals&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; pkill -SIGHUP signals&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; pkill -SIGINT signals
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This would be a catastrophic bug in an init system or process
supervisor (and/or something like
&lt;a href=&#34;https://github.com/joyent/containerpilot&#34;&gt;ContainerPilot&lt;/a&gt;, where it
actually was a bug in early versions). We need to catch &lt;code&gt;SIGWAIT&lt;/code&gt; to
reap zombie processes. It&#39;d also cause dropped signals for an
interactive terminal application, where we&#39;d probably masking
&lt;code&gt;SIGWINCH&lt;/code&gt; to detect terminal window size changes.&lt;/p&gt;
&lt;p&gt;But for most web applications this isn&#39;t a huge deal. Typically where
this bites us is if we have an orchestration layer that sends &lt;code&gt;SIGINT&lt;/code&gt;
or &lt;code&gt;SIGTERM&lt;/code&gt; for graceful shutdown and then kills the process
unceremoniously after a timeout. But there&#39;s some kind of automated
process that&#39;s picking up changes from the environment and firing
&lt;code&gt;SIGHUP&lt;/code&gt; to do a config reload. If we drop the graceful shutdown
signal because we&#39;re stuck in a config reload, then the orchestrator
sends an interrupt that the application ignores. After 10 seconds or
whatever your timeout is, the orchestration says &amp;quot;whelp, I give up&amp;quot;
and sends a &lt;code&gt;SIGKILL&lt;/code&gt;. And then our application drops in-flight
requests and users are unhappy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>git-send-email</title>
      <link>https://blog.0x74696d.com/posts/git-send-email/</link>
      <pubDate>Sat, 29 Aug 2020 12:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/git-send-email/</guid>
      <description>&lt;p&gt;Recently &lt;em&gt;The Register&lt;/em&gt; &lt;a href=&#34;https://www.theregister.com/2020/08/25/linux_kernel_email/&#34;&gt;published an
interview&lt;/a&gt;
with Microsoft&#39;s Sarah Novotny where she claimed that the Linux kernel
project&#39;s reliance on plain-text email was a barrier to entry for new
kernel developers.&lt;/p&gt;
&lt;p&gt;Predictably a bunch of folks showed up on Twitter to heap abuse and
gatekeep people&#39;s email clients, and just as predictably a lot of
well-meaning folks took the opposing side that because those people
were jerks, that Novotny was right. So I want to talk about both why
the structure of these kinds of arguments is such a disaster and why I
agree with Novotny&#39;s stated goals but think that she doesn&#39;t have much
of a solution to the problem.&lt;/p&gt;
&lt;h2 id=&#34;misaligned-goals&#34;&gt;Misaligned Goals&lt;/h2&gt;
&lt;p&gt;Let&#39;s address the jerks first because they&#39;re the least interesting
bit. Novotny&#39;s stated goals as Microsoft&#39;s representative to the Linux
Foundation board are to ensure the long term survival of the Linux
kernel project and in particular to ensure there&#39;s a flow of new
maintainers to the project. It should follow without question that for
there to be new maintainers, there needs to be a flow of new
contributors who eventually become experience contributors who can
take over from the old maintainers as they literally age-out of
working on the kernel full time. I can&#39;t think of any possible
good-faith argument against this goal, because it&#39;s rooted in the
reality that kernel developers are mortal.&lt;/p&gt;
&lt;p&gt;I&#39;m also going to put some words into Novotny&#39;s mouth here (in a
friendly sense) and suggest that in referring &amp;quot;developers who have
grown up in the last five or ten years&amp;quot; she&#39;s also looking to expand
the &lt;em&gt;demographics&lt;/em&gt; of the kernel project contributors. That&#39;s a worthy
goal!&lt;/p&gt;
&lt;p&gt;But whether or not she intended to imply that, I suspect that many of
the gatekeeping types &lt;em&gt;think&lt;/em&gt; she implied it. This is what sets up the
Twitter shitposting, because you have a very noisy group of people who
long-ago staked ground that they want tech to be the domain of cranky
white cis dudes (optionally with beards) and will jump at the
opportunity to fight about it. I find these people super frustrating
both because they&#39;re awful and because they suck all the air out of
the room from what could otherwise be adult conversations about the
best tactics. Unfortunately a lot of very smart and empathetic people
that I like get suckered into engaging in that conversation. You can&#39;t
reach these people, only freeze them out. (And hope that eventually
they&#39;ll do some work on themselves and be ready to join a culture that
is happy to embrace them again, but I&#39;m admittedly cynical about
that.)&lt;/p&gt;
&lt;h2 id=&#34;not-disinterested&#34;&gt;Not Disinterested&lt;/h2&gt;
&lt;p&gt;The second set of arguments you can have here is that Novotny is not a
disinterested party. She works for Microsoft, and previously worked at
Google. Both of these organizations have reputations for open source
malfeasance and those reputations are going to be reflected onto
anything she says.&lt;/p&gt;
&lt;p&gt;If you read the interview carefully, you&#39;ll find that Novotny is
talking in fairly broad strokes without really recommending anything
in particular. (This probably contributes to the focus of the
discussion on plain-text email and not maintainer succession) So if
you think that the reputation of Microsoft is well-deserved, you&#39;re
not likely to read between those lines in a way that assumes good
intent. Instead, it vaguely smells like another nefarious attempt at
&lt;a href=&#34;https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguish&#34;&gt;&amp;quot;embrace, extend,
extinguish&amp;quot;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A similar example might be if Linux Torvalds has something excitable
but borderline to say on the LKML. Because of his reputation as being
an asshole, if you&#39;re inclined to see him as a jerk you&#39;ll read what
he says uncharitably. Whereas if you&#39;re inclined to believe he&#39;s
trying to do the work of self-improvement, you may read it more
generously as enthusiastically penetrating questions to a colleague he
respects.&lt;/p&gt;
&lt;p&gt;Novotny&#39;s playing coy about hosting kernel development on GitHub
probably works against her here. We all know that&#39;s what we&#39;re talking
about, because there are no technically feasible alternatives for a
project of that scale. (Sorry GitLab.)&lt;/p&gt;
&lt;p&gt;In any case, while I&#39;m not particularly inclined to see Microsoft in a
good light, in this case I don&#39;t see much to be paranoid about. While
I&#39;m sure GitHub would love the reputational boost of hosting kernel
development, this is small potatoes in the grand scheme of things. It
wouldn&#39;t give Microsoft special control over the project that it
doesn&#39;t already have by its funding, board position, and many
development contributions.&lt;/p&gt;
&lt;h2 id=&#34;nobody-escapes-conways-law&#34;&gt;Nobody Escapes Conway&#39;s Law&lt;/h2&gt;
&lt;p&gt;If we get rid of misaligned goals or accusations of bad faith, that
leaves us with a discussion of tactics. This is where I suspect
Novotny&#39;s background at Google is influencing her to try to apply a
tooling fix to a cultural problem.&lt;/p&gt;
&lt;p&gt;Which is to say, &lt;code&gt;git-send-email&lt;/code&gt; is not the problem here.&lt;/p&gt;
&lt;p&gt;Daniel Vetter&#39;s 2017 post &lt;a href=&#34;https://blog.ffwll.ch/2017/08/github-why-cant-host-the-kernel.html&#34;&gt;Why Github can&#39;t host the Linux Kernel
Community&lt;/a&gt;
does a good job summarizing the distributed structure of the kernel
development project:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;No one (except Linus himself) is developing stuff on top of Linus
repository. Every subsystem, and often even big drivers, have their
own git repositories, with their own mailing lists to track
submissions and discuss issues completely separate from everyone
else.&lt;br/&gt;
...&lt;br/&gt;
But looking closer, its very, very far away from a single git
repository. Just looking at the upstream subsystem and driver
repositories gives you a few hundred. If you look at the entire
ecosystem, including hardware vendors, distributions, other
linux-based OS and individual products, you easily have a few
thousand major repositories, and many, many more in total.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As skilled as the kernel developers may be, nobody escapes Conway&#39;s
Law. The systems they have developed, &lt;em&gt;including git itself&lt;/em&gt;, are
reflections of the organization that created them.&lt;/p&gt;
&lt;p&gt;The kernel is not developed in the same way that Kubernetes is,
because it&#39;s not organized the same way. Kubernetes is largely run by
many committees (&amp;quot;SIGs&amp;quot;), befitting its origin as a corporate
controlled project. While Linux is developed largely via the
contributions of these same corporations, the technical governance
structure is one of distributed hierarchies.&lt;/p&gt;
&lt;p&gt;Someone looking to contribute to the kernel needs to understand the
kernel subsystem in question. They need to write professional-grade
C. They need to use the notoriously user-hostile &lt;code&gt;git&lt;/code&gt; source control
software. Given those heady requirements, I suspect that plain-text
email is not the barrier to entry that Novotny thinks it is. And
certainly compared to understanding the sprawling organization of the
project it seems like a tiny one.&lt;/p&gt;
&lt;p&gt;Search for &amp;quot;getting into linux kernel development&amp;quot; and the best page
you find is the
&lt;a href=&#34;https://www.kernel.org/doc/html/latest/process/howto.html&#34;&gt;kernel.org&lt;/a&gt;
page that gets you started with... kbuild, email patches, and coding
style? A less narrow search found the &lt;a href=&#34;https://www.kernel.org/doc/html/v5.7/process/development-process.html&#34;&gt;development
process&lt;/a&gt;
page which is better, but not exactly a welcome mat.&lt;/p&gt;
&lt;p&gt;This isn&#39;t a tooling problem, it&#39;s one of human communication. And
what I find especially frustrating about a focus on tooling is that
Novotny&#39;s employer is one of those uniquely positioned to contribute
to fixing the human problems.&lt;/p&gt;
&lt;p&gt;The huge corporate contributors like Microsoft, Google, and RedHat
should be building on-ramps to kernel development. They should be
producing on-boarding documentation, guides to how the project is
structured, and providing mentorship (and sponsorship!) for new kernel
developers. They should be ensuring that their own pipeline of kernel
contributors is diverse and that the contributors they employ are
building an inclusive culture within the LKML and other project
spaces. And they should be holding each other accountable for doing
the same.&lt;/p&gt;
&lt;p&gt;Telling El Reg the issue is plain text email only distracts from
solving the real problems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Locality</title>
      <link>https://blog.0x74696d.com/posts/locality/</link>
      <pubDate>Tue, 04 Jun 2019 01:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/locality/</guid>
      <description>&lt;p&gt;On Memorial Day weekend I grilled a steak, as one does. &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; A few days before, I took it out of the freezer and put it into the fridge to thaw on a plate. Late Sunday afternoon I took it out of the fridge and patted it down with a paper towel. Then I put it back in the fridge. Then I took it out and rubbed it down with oil and salt and pepper. Then I put it back in the fridge. I stepped outside to get the charcoals going. When I came back I found that my partner had seen the steak sitting in the fridge unattended and put it back into the freezer. I took the steak out of the freezer and back into the fridge. Then I took it out of the fridge and moved it to the grill. After a few minutes I &lt;s&gt;stretched this metaphor to its breaking point&lt;/s&gt; took the steak off the grill and put it back into the fridge. Then I took it out of the fridge and put it back on the grill to cook the other side. Then I put it back in the fridge. Finally, I took it out to serve.&lt;/p&gt;
&lt;p&gt;This is your data on serverless.&lt;/p&gt;
&lt;p&gt;In December a paper &lt;a href=&#34;https://arxiv.org/pdf/1812.03651.pdf&#34;&gt;&lt;em&gt;Serverless Computing: One Step Forward, Two Steps Back&lt;/em&gt;&lt;/a&gt; (Joseph M. Hellerstein, Jose Faleiro, Joseph E. Gonzalez, Johann Schleier-Smith, Vikram Sreekanti,
Alexey Tumanov and Chenggang Wu) discussed this problem at more depth and seriousness that I&#39;ve done here. The authors address missed opportunities in the current serverless landscape such as specialized hardware (ex. GPUs), but that&#39;s a matter of feature development and not inherent to the model as it currently exists.&lt;/p&gt;
&lt;p&gt;The more fundamental problem they illustrate is that serverless is a &amp;quot;data shipping architecture&amp;quot; where communication between tasks is via storage I/O. Instead of being able to take advantage of all the last couple decades&#39; worth of advances in distributed computing, we&#39;re relying on a giant blob of global state. This problem persists even if we assume that the various operational difficulties of deploying serverless can be resolved with better tooling. (I don&#39;t see any reason this shouldn&#39;t be the case, see companies like &lt;a href=&#34;https://www.iopipe.com/&#34;&gt;IOPipe&lt;/a&gt; for an example of the possibilities). But in the existing implementations of serverless, we can&#39;t get around the problem that your serverless functions are a sea of unstructured side-effects.&lt;/p&gt;
&lt;p&gt;In addition to semantics that&#39;ll make Haskell developers cry, the lack of data locality undermines mechanical sympathy. How can we we reason about performance when the underlying compute is so profoundly abstracted and your next &amp;quot;cache line&amp;quot; is an S3 API response 200ms away? This isn&#39;t so bad if you&#39;re a large cloud provider charging a premium for those milliseconds. But if performance is important to your workload (or perhaps you just care about the environmental impact of all that extraneous compute power), it&#39;s worth considering if the tradeoffs are worth it.&lt;/p&gt;
&lt;p&gt;There&#39;s an interesting historical note here in that only a short time ago the industry understood this problem of data locality, and this led to the Hadoop hype. In a typical map-reduce workflow, your data is distributed across HDFS and then your mapping computation happens physically co-located with the data. This hasn&#39;t ever been my particular area of expertise, but it seems that there were a couple of factors that contributed to the fizzling of the Hadoop hype. One is that it doesn&#39;t support update-in-place semantics, so you can&#39;t quite support arbitrary Unix applications. The second factor is the dominance of object storage in the form of S3 and the various upstack services that AWS has created on top of it. The pricing of S3 is aggressive relative to trying to build HDFS on top of instance storage or EBS, so if you&#39;re all-in on the cloud it&#39;s hard to make the economics work.&lt;/p&gt;
&lt;p&gt;A counterexample of this trend is Joyent&#39;s &lt;a href=&#34;https://github.com/joyent/manta&#34;&gt;Manta&lt;/a&gt;. They have an object store built on top of their Triton platform that allows you to instantiate a container (a SmartOS zone) directly &amp;quot;on&amp;quot; the objects in the object store. So you get a full Unix environment to perform compute on the objects without moving the data. Your ability to parallelize workloads is limited only by the replication factor and size of the storage cluster. Under the hood it&#39;s all built on ZFS, zones, and cleverly managed Postgres. It&#39;s really amazing technology and as a bonus it&#39;s open source!&lt;/p&gt;
&lt;p&gt;There are definitely a few barriers to Manta&#39;s wider adoption. Without Linux support for the compute zones, machine learning teams are less likely to adopt it. It doesn&#39;t support the S3 API so organizations potentially have a bunch of third-party tooling to recreate. While Manta is open source, it&#39;s decidedly not a standalone application but really a way of building an entire datacenter. So it can&#39;t be deployed onto AWS if you&#39;re already there. (Joyent does have an excellent cloud offering if you don&#39;t need much in the way of AWS upstack services.) And most importantly from the standpoint of serverless workflows, there&#39;s not yet a way to &amp;quot;watch&amp;quot; for events on Manta or get a changefeed as an end user; this could allow Lambda-like workflows.&lt;/p&gt;
&lt;aside&gt;Update June 6, 2019: only days after publishing this, Joyent announced they are shutting down their public cloud offering. Triton and Manta are still open source, and many of Joyent&#39;s enterprise customers are running on-prem. But sadly I have trouble imagining someone starting a new project on Triton following that announcement.&lt;/aside&gt;
&lt;p&gt;If you are a smaller cloud provider or just an organization struggling with problems of data locality in your data pipeline, you could do much worse than standing on the shoulders of giants and taking a look at Manta. Even if you&#39;re already all-in on AWS and/or Linux containerization schedulers like k8s or Nomad, there&#39;s an opportunity for a sufficiently motivated team&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; to take inspiration from Manta to build a system that brings better mechanical sympathy to serverless.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Yes, more cooking metaphors. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Which could include me, if you were to hire me to work on projects like this at your org! &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Mise-en-Place</title>
      <link>https://blog.0x74696d.com/posts/mise-en-place/</link>
      <pubDate>Mon, 03 Jun 2019 01:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/mise-en-place/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Mise-en-place is the religion of all good line cooks. Do not fuck with a line cook&#39;s &#39;meez&#39;  meaning his setup, his carefully arranged supplies of sea salt, rough-cracked pepper, softened butter, cooking oil, wine, backups, and so on.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;cite&gt; Anthony Bourdain, _Kitchen Confidential_&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.0x74696d.com/images/20190604/1200px-Mise_en_place_for_hot_station.jpg&#34; alt=&#34;mise-en-place&#34;&gt;&lt;/p&gt;
&lt;aside&gt;photo by Charles Haynes - &lt;a&gt;https://www.flickr.com/photos/haynes/500435491&lt;/a&gt;, CC BY-SA 2.0, &lt;a&gt;https://commons.wikimedia.org/w/index.php?curid=35488828&lt;/a&gt;&lt;/aside&gt;
&lt;p&gt;It probably comes off as pretentious and tone-deaf as a software engineer to compare anything we do at all to work in the food industry. Most of us have pretty cushy lives in comparison and are far removed from that kind of back-breaking manual labor.&lt;/p&gt;
&lt;p&gt;The sole extent of my experience in the food industry is one hot summer of getting up at 4am to open a coffee shop. I&#39;d make lattes for the Philly tourists and office workers until noon, coming home wired, scalded, and with coffee grounds embedded in my fingernails. My primary goal at that age was same as most, which meant trying to pick up the more lucrative closing shift where we served booze and stayed out late afterwards. In retrospect I was terrible at this job and didn&#39;t learn any of its essential lessons until much later in life.&lt;/p&gt;
&lt;h2 id=&#34;keep-the-plates-moving&#34;&gt;Keep the Plates Moving&lt;/h2&gt;
&lt;p&gt;That being said, &lt;em&gt;mise-en-place&lt;/em&gt; is an awesome metaphor for the day-to-day foundations of software engineering work.&lt;/p&gt;
&lt;p&gt;The goal of the production kitchen is to repeatably ship plate after plate at the expected quality level, night after night. The chef takes the measure of the results and adjusts the menu as market conditions (supply availability or customer demand) change. Does this sound familiar?&lt;/p&gt;
&lt;p&gt;This is all made possible by the kitchen team having that foundation of &lt;em&gt;mise&lt;/em&gt; so that they can focus on the work of production without the distraction of looking around for the salt after each dish.&lt;/p&gt;
&lt;p&gt;It is totally possible to ship software without doing it &lt;em&gt;well&lt;/em&gt; (as evidenced by... &lt;em&gt;*gestures broadly*&lt;/em&gt;). But the kitchen of our software development lifecycle can really only sing along when we&#39;ve done it. It&#39;s the work we do to make our work better.&lt;/p&gt;
&lt;p&gt;What am I really talking about here? In software development, your &lt;em&gt;mise&lt;/em&gt; is all the work that&#39;s not writing &amp;quot;production&amp;quot; code. It&#39;s the design document, the team style guide, a good Makefile, writing tests, and continuous integration and delivery. It&#39;s adding hooks for observability. It&#39;s writing good commit messages. It&#39;s writing good after action reviews and making sure they&#39;re shared across the org. All the work that enables our ability to focus and repeatably deliver quality products.&lt;/p&gt;
&lt;h2 id=&#34;stay-out-of-the-weeds&#34;&gt;Stay Out of the Weeds&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;If you let your mise-en-place run down, get dirty and disorganized, you&#39;ll quickly find yourself spinning in place and calling for backup. I worked with a chef who used to step behind the line to a dirty cook&#39;s station in the middle of a rush to explain why the offending cook was falling behind. He&#39;d press his palm down on the cutting board, which was littered with peppercorns, spattered sauce, bits of parsley, bread crumbs and the usual flotsam and jetsam that accumulates quickly on a station if not constantly wiped away with a moist side towel. &amp;quot;You see this?&amp;quot; he&#39;d inquire, raising his palm so that the cook could see the bits of dirt and scraps sticking to his chef&#39;s palm. &amp;quot;That&#39;s what the inside of your head looks like now.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;cite&gt; Anthony Bourdain, _Kitchen Confidential_&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I&#39;m pointedly avoiding the term &amp;quot;technical debt.&amp;quot; Not having your &lt;em&gt;mise-en-place&lt;/em&gt; together isn&#39;t just technical debt (although it&#39;s also that). It&#39;s cultural debt. Having a dirty working environment encourages more of the same. What&#39;s one more flaky integration test if half the tests are already flaky? And maybe it&#39;s not worth even writing that one. The on-call got woken up at 2am by an alarm that wasn&#39;t actionable. Ok, we&#39;ll mute that alarm for now and it&#39;ll be the next rotation&#39;s problem.&lt;/p&gt;
&lt;p&gt;All this noise is the enemy of deep work. It can lead your team to confuse urgency for importance. That cultural debt is a lot harder to pay down than the technical debt.&lt;/p&gt;
&lt;h2 id=&#34;expediting&#34;&gt;Expediting&lt;/h2&gt;
&lt;p&gt;Now, there is a particular personality in our industry who really loves to work on their &lt;em&gt;mise&lt;/em&gt; but at the expense of its purpose. This is the person who wants to have a 10000 word style guide with detailed rules for how to name variables and which RPC protocol to use in golang before a single line of code has been written. (What? No, this is a &amp;quot;totally&amp;quot; &amp;quot;hypothetical&amp;quot; &amp;quot;example&amp;quot;.)&lt;/p&gt;
&lt;p&gt;Hopefully in this situation your team can hold each other accountable to their goal. Otherwise you invariably end up with someone acting as expediter to kick everyone in the ass and get them moving. This creates a lot of hard feelings all around, and the team&#39;s manager either looks like an ass or ineffectual, depending on how that went down.&lt;/p&gt;
&lt;h2 id=&#34;home-cooking-vs-professional-cooking&#34;&gt;Home Cooking vs Professional Cooking&lt;/h2&gt;
&lt;p&gt;Another way in which this metaphor is helpful is differentiating between the home cook and the professional line cook. When we&#39;re puttering around at home in our kitchens, having a decent &lt;em&gt;mise-en-place&lt;/em&gt; can make the work more pleasant. But it&#39;s not critical to completing a meal. There&#39;s no customer who&#39;s going to walk out if their entre doesn&#39;t land by 7pm sharp. There&#39;s no requirement for repeatability. With sufficient dedication you can try a new dish every day and if a few flop, no one is going to stop you from cooking for them anymore.&lt;/p&gt;
&lt;p&gt;We&#39;re the home cook when we&#39;re writing software for ourselves. When we rework our blog CSS yet again. When we learn a new language. When we scratch an itch about an open source project we use.&lt;/p&gt;
&lt;p&gt;But for the professional, speed and repeatability are vital. Having our &lt;em&gt;mise&lt;/em&gt; tight is a requirement for maintaining quality and velocity.&lt;/p&gt;
&lt;h2 id=&#34;what-do-we-value&#34;&gt;What Do We Value?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;As a cook, your station, and its condition, its state of readiness, is an extension of your nervous system...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;cite&gt; Anthony Bourdain, _Kitchen Confidential_&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We don&#39;t value the &lt;em&gt;mise-en-place&lt;/em&gt; processes in and of themselves, but we value the result they enable. But those results aren&#39;t merely the artifacts  the software we write  but also the way we feel about our work. We shouldn&#39;t dismiss the value of flow state, of feeling like the work we&#39;re doing is the best we can do. And we shouldn&#39;t dismiss that our work can carry intrinsic spiritual or emotional value.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Checkpointing Failure</title>
      <link>https://blog.0x74696d.com/posts/checkpointing-failure/</link>
      <pubDate>Sun, 17 Feb 2019 01:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/checkpointing-failure/</guid>
      <description>&lt;p&gt;The conversation goes something like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Them: &amp;quot;Our service can&#39;t be autoscaled, run on spot instances, or have its host restarted at random because it runs long-running tasks.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Me: &amp;quot;Are the tasks idempotent?&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Them: &amp;quot;No, but they&#39;re checkpointed.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Me: &amp;quot;Even if we don&#39;t autoscale, run on spot instances, or ever update the host, the host can randomly fail at any time.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Them: &amp;quot;Yes, but that&#39;s less often so it&#39;s ok. Throughput is ok if failure happens rarely.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Me: &amp;quot;But you have a bug if the tasks can&#39;t be safely retried.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Them: &amp;quot;I told you, we checkpoint it.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you have lots of experience with batch workloads, there&#39;s probably nothing new here for you. But I had three similar conversations about this problem recently, so let&#39;s look into it.&lt;/p&gt;
&lt;p&gt;The defining characteristic of the kinds of tasks we&#39;re talking about here is that they modify external state: they reserve a table at restaurant, they update the follower count in your social media network, they cause your book order to be shipped. These tasks are typically created by publishing to a queue which our workers are consuming, or they are generated on a schedule via something like cron.&lt;/p&gt;
&lt;p&gt;There are two primary attributes we&#39;re concerned with here. Tasks must be &lt;strong&gt;correct&lt;/strong&gt; and they must have acceptable &lt;strong&gt;throughput.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;By correctness, we mean that the task gets the right answer and does the right work. But because these tasks modify state, correctness also implies &lt;strong&gt;idempotency&lt;/strong&gt;. That is, if we have to retry them because the task fails for reasons out of our control, it should be safe to do so. We should not, for example, cause two of the same book to be shipped to you.&lt;/p&gt;
&lt;p&gt;By throughput, we mean the performance of the task. Specifically in this case the number of tasks that can be processed by the worker. Tasks can vary quite a bit in how long they take, but if we have failures which cause us to start over, our throughput goes down. To reduce the amount of throughput lost, we can rely on &lt;strong&gt;checkpointing&lt;/strong&gt;: we save our work in the middle of the job, allowing us to pick up where we left off with only the work between checkpoints lost.&lt;/p&gt;
&lt;p&gt;The external force on these two values is the &lt;strong&gt;error rate&lt;/strong&gt;. This is how often a task fails, for any reason. Even if the developer never writes a bug, perhaps the task has a network timeout. Perhaps the infrastructure team is making a kernel update and restarts the host. Perhaps the Kubernetes cluster reschedules the job. Or perhaps an electrical fire burns down the rack of hosts, sparing them the indignity of running Kubernetes.&lt;/p&gt;
&lt;p&gt;In the conversation I had above, the developer is conflating the purpose of the two knobs of idempotency and checkpointing. A developer can tune the throughput of their tasks by adjusting the length of steps taken between checkpoints relative to the rate of unexpected errors. But increasing the rate of checkpoints does nothing for correctness.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;And as we&#39;ll see below, increasing the rate of checkpoints can very easily damage correctness.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I&#39;ve worked up a simple model to demonstrate the effect the two knobs of idempotency and checkpoint rate have on both correctness and throughput, at various error rates. This model ignores concurrency for clarity, but concurrent tasks make the correctness problem even more important to solve. You can follow along with the code &lt;a href=&#34;https://github.com/tgross/blog.0x74696d.com/blob/trunk/static/_code/checkpointing/checkpoint.py&#34;&gt;on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We run each set of parameters through our model for 100,000 &amp;quot;ticks&amp;quot;. For each tick through our model, our task updates a pair of counters in a SQL database. In the middle of doing so, there is a small chance that the update fails. Each model reports the values for each counter. The difference in value between the two counters (if any) we&#39;ll refer to as the &lt;strong&gt;drift&lt;/strong&gt; and it reflects correctness. The maximum value of the counter reflects the throughput. In a perfect world where there is a 0% error rate, both counters will have a value of 100,000.&lt;/p&gt;
&lt;p&gt;Let&#39;s look at our idempotent task processor first.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;idempotent_task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;checkpoint_steps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;err_rate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tick&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;event_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;cur&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cursor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;cur&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;execute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;INSERT OR REPLACE INTO counterA VALUES (?)&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;event_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,))&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;maybe_error&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;err_rate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;cur&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;execute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;INSERT OR REPLACE INTO counterB VALUES (?)&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;event_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,))&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;maybe_checkpoint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tick&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;checkpoint_steps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;event_id&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;Exception&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rollback&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;event_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We pass the &lt;code&gt;event_id&lt;/code&gt; into the task and increment it upon success. The &lt;code&gt;event_id&lt;/code&gt; is returned whether or not it has been incremented, so the next iteration will retry failed events. Additionally, we update both counters in a single &lt;strong&gt;atomic transaction&lt;/strong&gt; so that we can&#39;t have partial updates. Note that atomicity and idempotency aren&#39;t the same thing! But you can&#39;t have idempotency without atomicity if you make multiple updates in a given task.&lt;/p&gt;
&lt;p&gt;An alternative to retrying events would be to simply drop work that fails and not retry it. If our interest in the event is bound by time, this might be correct behavior. For example, if the event was a location update of our moving rideshare car, we might decide to ignore a stale update in favor of simply waiting for the next one.&lt;/p&gt;
&lt;p&gt;Now let&#39;s take a look at our non-idempotent processor.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;non_idempotent_task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;checkpoint_steps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;err_rate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tick&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;event_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;cur&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cursor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;cur&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;execute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;INSERT OR REPLACE INTO counterA VALUES (?)&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;event_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,))&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;maybe_checkpoint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tick&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;checkpoint_steps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;maybe_error&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;err_rate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;cur&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;execute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;INSERT OR REPLACE INTO counterB VALUES (?)&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;event_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,))&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;maybe_checkpoint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tick&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;checkpoint_steps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;event_id&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;Exception&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rollback&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;event_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This non-idempotent task represents a common source of bugs. We&#39;ve tried to make it idempotent by using the &lt;code&gt;event_id&lt;/code&gt; as we did in our previous task. But because this isn&#39;t an atomic transaction, each table can see a different set of events! The most common way this happens in my experience is &lt;strong&gt;write skew&lt;/strong&gt;: an application that reads from the database, and then writes values back based on those values without taking into account concurrent updaters.&lt;/p&gt;
&lt;p&gt;I&#39;ve run these two tasks with error rates ranging up to 2%. That rate is perhaps pathological, but consider a task with a 20-minute long step between checkpoints. If its host is restarted once per week for kernel updates that&#39;s a 2% &amp;quot;failure rate&amp;quot; per host, assuming nothing else goes wrong. The other parameter is checkpoint steps ranging from 1 (checkpoint every tick) to 11 (checkpoint every 11 ticks).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.0x74696d.com/images/20190217/plot.png&#34; alt=&#34;diagram&#34;&gt;&lt;/p&gt;
&lt;p&gt;The top graph measures throughput. We can see that as the error rate increases, the throughput decreases as we&#39;d expect. We can also see that as the frequency of checkpointing goes up, the throughput goes up. For non-idempotent tasks that checkpoint after every step, we can reach very nearly 100,000. But for each pair of idempotent and non-idempotent tasks at each value of the checkpoint steps parameter, we see that the idempotent tasks fare worse in throughput performance.&lt;/p&gt;
&lt;p&gt;The bottom graph measures correctness. At the bottom we see a single dotted line representing all the idempotent tasks together: they have no drift between the counters! But for non-idempotent tasks, we can see that as they checkpoint more frequently, not only does the checkpointing not help their correctness, but it compounds the errors they make.&lt;/p&gt;
&lt;p&gt;What this demonstrates is that correctness cannot be truly solved by improving the failure rate of your infrastructure. If you want the wrong answer quickly, feel free to checkpoint without idempotency. But if you want software that works, your tasks need to be idempotent.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Debugging Python Containers in Production</title>
      <link>https://blog.0x74696d.com/posts/debugging-python-containers-in-production/</link>
      <pubDate>Thu, 03 May 2018 01:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/debugging-python-containers-in-production/</guid>
      <description>&lt;p&gt;We all figure out our first Python bugs by sprinkling some &lt;code&gt;print&lt;/code&gt; statements over our code. As we gain experience, our debugging toolbox becomes richer and we can figure out harder bugs in development. But production systems provide a different kind of challenge, and this challenge is amplified when we try to debug in a containerized environment. We need to be able to debug running code safely, without impacting performance or interfering with the user experience.&lt;/p&gt;
&lt;p&gt;Some of the most powerful tools like debuggers or eBPF are the hardest to get working with Python containers, so in this post I&#39;ll cover methods to build Python containers for improved instrumentation and debugging. I gave a talk covering most of this content &lt;a href=&#34;https://www.meetup.com/phillypug/events/244306771/&#34;&gt;Philadelphia Python Users Group (PhillyPUG)&lt;/a&gt; last November. The original talk covered a bunch of material on logging but I&#39;ll revisit that in an upcoming post.&lt;/p&gt;
&lt;h2 id=&#34;groundwork&#34;&gt;Groundwork&lt;/h2&gt;
&lt;p&gt;Let&#39;s first assume that you&#39;ve grabbed all the low-hanging fruit. You&#39;re collecting structured logs or events from your applications in a centralized location like &lt;a href=&#34;https://www.elastic.co/elk-stack&#34;&gt;Elasticsearch&lt;/a&gt; or &lt;a href=&#34;https://honeycomb.io/&#34;&gt;Honeycomb.io&lt;/a&gt;. You&#39;re sending unhandled exceptions to something like &lt;a href=&#34;https://sentry.io/welcome/&#34;&gt;Sentry&lt;/a&gt;. If you have a web application, you&#39;re tagging incoming web requests at the edge with something like &lt;a href=&#34;https://www.nginx.com/blog/application-tracing-nginx-plus/&#34;&gt;Nginx request IDs&lt;/a&gt;. You can get really far with that! But it doesn&#39;t give you a detailed insight into how the application is behaving &amp;quot;under the hood&amp;quot;, particularly in the cases where the application is failing in a way that isn&#39;t already known. &lt;a href=&#34;https://youtu.be/AdMqCUhvRz8?t=1215&#34;&gt;Bryan Cantrill&lt;/a&gt; calls these &amp;quot;implicit failure&amp;quot; modes.&lt;/p&gt;
&lt;p&gt;With Python in particular, you can get insight into a lot of the application behavior with tools like &lt;a href=&#34;https://docs.newrelic.com/docs/agents/python-agent/getting-started/introduction-new-relic-python&#34;&gt;NewRelic&lt;/a&gt;. But this is incredibly expensive to deploy across your whole production footprint, it can&#39;t really help with crashed applications, and it can&#39;t look into the Python interpreter or operating system underneath your code. I also find that the expense means that it doesn&#39;t get used in development or testing environments, and that makes for a gap in understanding.&lt;/p&gt;
&lt;p&gt;The tools I&#39;ll discuss below do require some one-time up-front work, but the payoffs are enormous. First, to use native core dumps you need debugging symbols for Python. To use eBPF on Linux, you need to be on a modern Linux kernel (4.0+, or whatever frankenkernel RedHat is shipping these days). To use &lt;code&gt;usdt&lt;/code&gt; probes for Python you need to be on Python 3.6+. But I&#39;ve found most Linux distributions are not compiling-in the &lt;code&gt;usdt&lt;/code&gt; probes, including the various Docker containers that ship Python. So we&#39;re going to want to build our own Python. Don&#39;t worry! This is much easier than it sounds!&lt;/p&gt;
&lt;h2 id=&#34;building-your-python&#34;&gt;Building Your Python&lt;/h2&gt;
&lt;p&gt;The Docker Hub has a &lt;a href=&#34;https://store.docker.com/images/python&#34;&gt;Python image&lt;/a&gt; in its library. We need to slightly modify that build and make sure it&#39;s part of our continuous integration system. The source for the Dockerfiles is &lt;a href=&#34;https://github.com/docker-library/python/tree/master&#34;&gt;on GitHub&lt;/a&gt;. We only care about Python 3.6 and above.&lt;/p&gt;
&lt;aside&gt;Addendum (Feb 2019): I submitted a pull request to the Docker library (&lt;a href=&#34;https://github.com/docker-library/python/pull/366&#34;&gt;PR #366&lt;/a&gt;) for compiling in &lt;code&gt;usdt&lt;/code&gt; hooks. The change was benchmarked using Python&#39;s own benchmarking suite. Although many of the benchmarks don&#39;t show a significant difference between &lt;code&gt;--with-dtrace&lt;/code&gt; and not, &lt;strong&gt;26 of the 60 tests show a 5%-17% performance hit&lt;/strong&gt;, even without an active trace. This is probably not the approach you want. For live profiling in production you might instead want to check out &lt;a href=&#34;https://github.com/benfred/py-spy&#34;&gt;&lt;code&gt;py-spy&lt;/code&gt;&lt;/a&gt;&lt;/aside&gt;
&lt;p&gt;Python is written in C, and like many C applications under Unix it&#39;s built via Autotools. A &lt;code&gt;configure&lt;/code&gt; step takes a Makefile template and some parameters, and generates a Makefile that we call &lt;code&gt;make&lt;/code&gt; on to build the software. We want to alter the parameters that the Docker build is using to add debugging symbols (the &lt;code&gt;--with-pydebug&lt;/code&gt; flag) and tracepoints (the &lt;code&gt;--with-dtrace&lt;/code&gt; flag). So for example as of this writing, we&#39;d be adding these flags to the template used for the &lt;code&gt;docker/python:3.6-slim&lt;/code&gt; version &lt;a href=&#34;https://github.com/docker-library/python/blob/ba5711fb564133bf9c8b870b431682a4db427219/Dockerfile-slim.template#L61-L67&#34;&gt;here&lt;/a&gt;. We also need to include the installation of &lt;code&gt;systemtap-sdt-dev&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-diff&#34; data-lang=&#34;diff&#34;&gt;&lt;span class=&#34;gh&#34;&gt;index 6799174..16dbbf0 100644
&lt;/span&gt;&lt;span class=&#34;gh&#34;&gt;&lt;/span&gt;&lt;span class=&#34;gd&#34;&gt;--- a/Dockerfile-debian.template
&lt;/span&gt;&lt;span class=&#34;gd&#34;&gt;&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+++ b/Dockerfile-debian.template
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;&lt;/span&gt;&lt;span class=&#34;gu&#34;&gt;@@ -19,6 +19,7 @@ ENV PYTHON_VERSION %%PLACEHOLDER%%
&lt;/span&gt;&lt;span class=&#34;gu&#34;&gt;&lt;/span&gt; RUN set -ex \
         &amp;amp;&amp;amp; buildDeps=&amp;#39; \
                dpkg-dev \
&lt;span class=&#34;gi&#34;&gt;+               systemtap-sdt-dev \
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;&lt;/span&gt;                tcl-dev \
                tk-dev \
         &amp;#39; \
&lt;span class=&#34;gu&#34;&gt;@@ -43,6 +44,8 @@ RUN set -ex \
&lt;/span&gt;&lt;span class=&#34;gu&#34;&gt;&lt;/span&gt;                --with-system-expat \
                --with-system-ffi \
                --without-ensurepip \
&lt;span class=&#34;gi&#34;&gt;+               --with-pydebug \
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;+               --with-dtrace \
&lt;/span&gt;&lt;span class=&#34;gi&#34;&gt;&lt;/span&gt;       &amp;amp;&amp;amp; make -j &amp;#34;$(nproc)&amp;#34; \
       &amp;amp;&amp;amp; make install \
       &amp;amp;&amp;amp; ldconfig \
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &amp;quot;best&amp;quot; way to accomplish this is going to depend a lot on how you build the rest of your software. But the overall steps you need are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fork the &lt;a href=&#34;https://github.com/docker-library/python&#34;&gt;https://github.com/docker-library/python&lt;/a&gt; and add the patch above to any of the templates you need.&lt;/li&gt;
&lt;li&gt;Have your CI system build the container images on a regular basis. You want to make sure you&#39;re pulling in any changes to both Python and the base Debian or Alpine image you&#39;re using.&lt;/li&gt;
&lt;li&gt;Have the output of the CI system be a push to your organization&#39;s private Docker registry (or even a public one if you don&#39;t mind sharing).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can find my fork at &lt;a href=&#34;https://github.com/tgross/docker-python&#34;&gt;https://github.com/tgross/docker-python&lt;/a&gt;. I&#39;m using TravisCI to create a weekly build of Python 3.6 and 3.7 for Debian and pushing it to the Docker Hub under &lt;a href=&#34;https://hub.docker.com/r/0x74696d/python/&#34;&gt;https://hub.docker.com/r/0x74696d/python/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you aren&#39;t using containers, don&#39;t have immutable infrastructure, and deploy your software via &lt;code&gt;git pull&lt;/code&gt; in &lt;code&gt;ssh&lt;/code&gt; in a for loop, then you&#39;ll probably want to do something like the following instead. This assumes you&#39;re on a Debian-based distro like Ubuntu and that you have a clone of the Python source code handy:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# juuuuust a couple of dependencies...&lt;/span&gt;
sudo apt install &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    build-essential libssl-dev zlib1g-dev &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    libncurses5-dev libncursesw5-dev libreadline-dev &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    libsqlite3-dev libgdbm-dev libdb5.3-dev libbz2-dev &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    libexpat1-dev liblzma-dev tk-dev &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    systemtap-sdt-dev

./configure &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    --with-pydebug &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    --with-dtrace &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    --enable-loadable-sqlite-extensions &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    --enable-shared &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    --with-system-expat &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    --with-system-ffi &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    --without-ensurepip

make
make &lt;span class=&#34;nb&#34;&gt;test&lt;/span&gt;
sudo make install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;debugging-from-sidecars&#34;&gt;Debugging From Sidecars&lt;/h2&gt;
&lt;p&gt;Container images don&#39;t typically include debugging tools. They add a lot to the image size, but they also require root-like privileges (ex. &lt;code&gt;ptrace&lt;/code&gt;, &lt;code&gt;CAP_SYSADMIN&lt;/code&gt;) and the whole point of a container is that you can run it with reduced privileges. So typically you&#39;ll debug a container either from the host (if you have access to the host) or from a &amp;quot;swiss army knife&amp;quot; sidecar container like the one you can find at &lt;a href=&#34;https://github.com/tgross/swiss-army-knife&#34;&gt;https://github.com/tgross/swiss-army-knife&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Dockerfile&#34; data-lang=&#34;Dockerfile&#34;&gt;&lt;span class=&#34;c&#34;&gt;# swiss-army-knife container for debugging as side-car&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;FROM&lt;/span&gt;&lt;span class=&#34;s&#34;&gt; ubuntu:16.04&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# add whatever tools you want here&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;RUN&lt;/span&gt; apt-get update &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get install -y &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;       gdb &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;       strace &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;       tcpdump &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;       linux-tools &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;       software-properties-common &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;       apt-transport-https &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;       ca-certificates &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;       curl &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;       jq &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; rm -rf /var/lib/apt/lists/*&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;RUN&lt;/span&gt; add-apt-repository &lt;span class=&#34;s2&#34;&gt;&amp;#34;deb [trusted=yes] https://repo.iovisor.org/apt/xenial xenial-nightly main&amp;#34;&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get update &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get install -y --allow-unauthenticated bcc-tools &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; rm -rf /var/lib/apt/lists/*&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In either case you need to be aware of process namespaces. When you run a process in a container, it can&#39;t see all the other processes running on the host. In our Python container, the first process in the process tree (PID1) is typically going to be Python. Whereas PID1 on the container host is &lt;code&gt;systemd&lt;/code&gt; or some other init system. You need to know which view of the process tree you have when you pass the process ID to your debugging tools.&lt;/p&gt;
&lt;p&gt;If we look at the process tree from the host we get one list of processes:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ps afx

 PID COMMAND
...
1155 /usr/bin/dockerd -H fd://
1350 \_ docker-containerd -l unix:///var/run/docker/libcontainerd/docker-containe
22176 | \_ docker-containerd-shim a1e9578bfc58fb130a8b02fb413fc1579a4885a3fa0751
22193 | | \_ /usr/local/bin/python /usr/local/bin/gunicorn --name myapp
31786 | | \_ /usr/local/bin/python /usr/local/bin/gunicorn --name myapp
  479 | | \_ /usr/local/bin/python /usr/local/bin/gunicorn --name myapp
22879 | \_ docker-containerd-shim 6b6e053851cabc2e257e79ef130c140132d30d935e194b
22896 | \_ /usr/local/bin/python /usr/local/bin/gunicorn --name anotherapp
 3965 | \_ /usr/local/bin/python /usr/local/bin/gunicorn --name anotherapp
 4153 | \_ /usr/local/bin/python /usr/local/bin/gunicorn --name anotherapp
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Whereas if we look at the process tree from inside the container we&#39;ll get a different list:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker exec -it 6b6e053851ca ps -ef

 PID COMMAND
   1 {gunicorn} /usr/local/bin/python /usr/local/bin/gunicorn --name myapp
3446 {gunicorn} /usr/local/bin/python /usr/local/bin/gunicorn --name myapp
3453 {gunicorn} /usr/local/bin/python /usr/local/bin/gunicorn --name myapp
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If we want to run the eBPF tool &lt;code&gt;pythoncalls&lt;/code&gt; (see below) from the host, we need to use the PID from the point-of-view of the host: &lt;code&gt;sudo /usr/share/bcc/tools/pythoncalls 479&lt;/code&gt;. If we want to run this from a sidecar container, we need to use the container&#39;s view of the PID tree, share the process and network namespace, and give our sidecar elevated privileges for debugging:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;docker run -it &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    --pid&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;container:6b6e053851ca &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    --net&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;container:6b6e053851ca &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    --cap-add sys_admin &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    --cap-add sys_ptrace &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    swiss-army-knife &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    /usr/share/bcc/tools/pythoncalls -p &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;fatal-failure&#34;&gt;Fatal Failure&lt;/h2&gt;
&lt;p&gt;A fatal failure is one in which the process dies. This can be explicit  the program has an instruction that tells it to exit because it can&#39;t safely continue. Or it can be implicit  the program can&#39;t continue and crashes unexpectedly (for example, with a segfault or Python traceback). While fatal failure is unfortunate from the perspective of the user, it&#39;s often much easier to debug.&lt;/p&gt;
&lt;p&gt;The reason is that whether implicit or explicit, fatal failure allows for post-mortem debugging. We can start with the fatal state (a core dump), and move it off the production environment into our development environment where it can be examined with a lot less pressure. We use tools (our debugger) to reason backwards from the fatal state to a root technical cause. (Yes, yes, I realize there&#39;s no such thing as &amp;quot;root cause&amp;quot; in a complex socio-technical system. We&#39;re talking about the root &lt;em&gt;technical&lt;/em&gt; cause here.) The nice thing about this is that so long as the state was preserved we can typically discover the cause after a single failure.&lt;/p&gt;
&lt;p&gt;Python has its &lt;code&gt;pdb&lt;/code&gt; debugger, but doesn&#39;t have a facility for dumping Python interpreter state to use it offline. If you attach &lt;code&gt;pdb&lt;/code&gt; to a running process, it halts the process (which your users will not like), but you can&#39;t use it to debug post-mortem either. A Python traceback is only serializable in the trivial sense (dump to structured text), which is what services like Sentry use. Fortunately we can get core dumps from Python that are usable in the GNU debugger &lt;code&gt;gdb&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When the Python interpreter receives a &lt;code&gt;SIGABRT&lt;/code&gt; signal, it dumps the interpreter&#39;s memory to a core file on disk. On Linux we can use &lt;code&gt;gdb&lt;/code&gt; to read this core dump just as we would any other program. But what&#39;s cool about Python being interpreted is that your Python source code is all in the interpreter&#39;s memory, so &lt;code&gt;gdb&lt;/code&gt; has some extensions that let us debug into the Python application code just as we would the interpreter.&lt;/p&gt;
&lt;p&gt;Under normal circumstances, Python won&#39;t dump core. We can send the &lt;code&gt;kill&lt;/code&gt; signal to it manually, but there&#39;s another option  we can force Python to dump core on uncaught exception. I would only recommend this approach if you have good test coverage and are generally confident in your team&#39;s ability to write code that rarely crashes, as core dumps can get really large and eat up all your disk space unless you have something like &lt;a href=&#34;https://github.com/joyent/manta-thoth&#34;&gt;Joyent&#39;s Thoth&lt;/a&gt; to move them off-disk to shared object storage. Here&#39;s how you&#39;d add this to something like a Django middleware:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;logging&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;logger&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;logging&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;getLogger&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;vm&#34;&gt;__name__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;AbortOnUncaughtExceptionMiddleware&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;object&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_response&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_response&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__call__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;request&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_response&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;request&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;process_exception&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;request&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;exception&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;logger&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;error&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exception&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;abort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This causes the application to crash and core dump if an exception wasn&#39;t handled. You probably want this to be the last middleware that gets called (so first in the list for Django) so that you can catch things like HTTP 404s more gracefully. Of course you&#39;ll also need your supervisor (&lt;code&gt;systemd&lt;/code&gt; or similar) to restart the process after it crashes.&lt;/p&gt;
&lt;p&gt;On &lt;code&gt;systemd&lt;/code&gt;-based systems, core dumps are handled by &lt;code&gt;coredumpctl&lt;/code&gt;. We can use &lt;code&gt;coredumpctl&lt;/code&gt; to output to a file which we&#39;ll then move to our development environment. Here we&#39;re taking the first python3.6 dump listed by &lt;code&gt;coredumpctl&lt;/code&gt; and outputting it to the file &lt;code&gt;api.coredump&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ coredumpctl list
TIME PID UID GID SIG PRESENT EXE
Wed 2017-11-29 18:06:08 UTC 7858 0 0 6 * /usr/local/bin/python3.6
Wed 2017-11-29 18:06:18 UTC 7872 0 0 6 * /usr/local/bin/python3.6
Wed 2017-11-29 18:06:25 UTC 7881 0 0 6 * /usr/local/bin/python3.6
Wed 2017-11-29 18:07:21 UTC 7890 0 0 6 * /usr/local/bin/python3.6
Wed 2017-11-29 18:07:29 UTC 7914 0 0 6 * /usr/local/bin/python3.6

$ sudo coredumpctl -o api.coredump dump /usr/local/bin/python3.6
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once we have the core dump locally, we can load it into &lt;code&gt;gdb&lt;/code&gt; and import the Python-specific tools to list source code, move up and down the stack, read Python backtraces, and print the values of variables. For a detailed treatment of using the &lt;code&gt;gdb&lt;/code&gt; debugging tools see &lt;a href=&#34;https://devguide.python.org/gdb/&#34;&gt;https://devguide.python.org/gdb/&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ PYTHONPATH=/src/cpython/Tools/gdb gdb python3 api.coredump
...
(gdb) python import libpython
(gdb) py-list
  11        def __call__(self, request):
  12            return self.get_response(request)
  13
  14        def process_exception(self, request, exception):
  15            logger.error(exception)
 &amp;gt;16            os.abort()

(gdb) py-up
(gdb) py-locals
self = &amp;lt;AbortOnUncaughtExceptionMiddleware(get_response=&amp;lt;function at remote 0x7fc98848d4a8&amp;gt;) at remote 0x7fc9884b64d0&amp;gt;
request = &amp;lt;WSGIRequest(environ={&#39;wsgi.errors&#39;: &amp;lt;WSGIErrorsWrapper(streams=[&amp;lt;_io.TextIOWrapper at remote 0x7fc990140898&amp;gt;]) at remote 0x7fc9883c25a0&amp;gt;, &#39;wsgi.version&#39;: (1, 0), &#39;wsgi.multithread&#39;: False, &#39;wsgi.multiprocess&#39;: False, &#39;wsgi.run_once&#39;: False, &#39;wsgi.file_wrapper&#39;: &amp;lt;type at remote 0x140a698&amp;gt;, &#39;SERVER_SOFTWARE&#39;: &#39;gunicorn/19.7.1&#39;, &#39;wsgi.input&#39;: &amp;lt;Body(reader=&amp;lt;LengthReader(unreader=&amp;lt;SocketUnreader(buf=&amp;lt;_io.BytesIO at remote 0x7fc9883c01f0&amp;gt;, sock=&amp;lt;socket at remote 0x7fc9883af3b8&amp;gt;, mxchunk=8192) at remote 0x7fc98ace5c88&amp;gt;, length=0) at remote 0x7fc9883c26d8&amp;gt;, buf=&amp;lt;_io.BytesIO at remote 0x7fc9883c0410&amp;gt;) at remote 0x7fc9883c2740&amp;gt;, &#39;gunicorn.socket&#39;: &amp;lt;...&amp;gt;, &#39;REQUEST_METHOD&#39;: &#39;GET&#39;, &#39;QUERY_STRING&#39;: &#39;&#39;, &#39;RAW_URI&#39;: &#39;/histo/10/-1&#39;, &#39;SERVER_PROTOCOL&#39;: &#39;HTTP/1.1&#39;, &#39;HTTP_HOST&#39;: &#39;localhost:8000&#39;, &#39;HTTP_USER_AGENT&#39;: &#39;curl/7.47.0&#39;, &#39;HTTP_ACCEPT&#39;: &#39;*/*&#39;, &#39;wsgi.url_scheme&#39;: &#39;http&#39;, &#39;REMOTE_ADDR&#39;: &#39;127.0.0.1&#39;, &#39;REMOTE_PORT&#39;: &#39;55272&#39;, &#39;SERVER_NAME&#39;: &#39;127.0.0.1&#39;, &#39;SERVER_PORT&#39;: &#39;8000&#39;, &#39;PATH_INFO&#39;: &#39;/histo/10/-1&#39;, &#39;SCRIPT_NAME&#39;: &#39;&#39;}, p...(truncated)
exception = Exception(&#39;uh oh&#39;,))
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;non-fatal-failure&#34;&gt;Non-Fatal Failure&lt;/h2&gt;
&lt;p&gt;In contrast to fatal failures, non-fatal failures are sometimes the hardest problems to solve. These are the &amp;quot;unknown unknowns&amp;quot; of software engineering. Maybe your application is writing corrupted data. Maybe your application mysteriously runs slowly or freezes every few minutes. Maybe your application unexpectedly drops network connections. None of this is magic!&lt;/p&gt;
&lt;p&gt;These kinds of problems are often impossible to replicate in a development environment, especially when we&#39;re talking about the kinds of distributed systems that tend to pop up when we&#39;re working with containers. We need &lt;em&gt;in-vivo&lt;/em&gt; analysis. And that means using tools like DTrace (for Unix) or eBPF (the closest Linux equivalent). Because for better or worse most folks are deploying production on Linux, we&#39;ll talk about eBPF here. The general concepts are similar to DTrace but DTrace is much more mature and frankly nicer to work with.&lt;/p&gt;
&lt;p&gt;The Linux kernel includes a sandboxed bytecode interpreter that was originally created for IP tables filtering (Berkeley Packet Filter or BPF). In the 3.15+ kernel this bytecode interpreter has been extended allow user-defined programs to instrument a live system with minimal performance impact. To create these user-defined programs, we can use the &lt;a href=&#34;https://github.com/iovisor/bcc&#34;&gt;BCC&lt;/a&gt; toolkit. Programs are written in Python (or Lua) and compiled using LLVM to the eBPF bytecode. The eBPF programs read kernel instrumentation (kprobes) or user statically-defined trace points (&lt;code&gt;usdt&lt;/code&gt;). What&#39;s really cool is that the outputs of the program are stored in buffers shared between kernel space and user space, so there&#39;s no inefficient copying of the data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.0x74696d.com/images/20180503/eBPF-diagram.png&#34; alt=&#34;eBPF&#34;&gt;&lt;/p&gt;
&lt;p&gt;See also the &lt;a href=&#34;http://man7.org/linux/man-pages/man2/bpf.2.html&#34;&gt;bpf(2) man page&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The BCC toolkit comes with a ton of useful example tools. Want to sniff SSL traffic before the OpenSSL library encrypts it? Try &lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/tools/sslsniff.py&#34;&gt;&lt;code&gt;sslsniff.py&lt;/code&gt;&lt;/a&gt;. Want to figure out your DNS lookup latency? Try &lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/tools/gethostlatency.py&#34;&gt;&lt;code&gt;gethostlatency.py&lt;/code&gt;&lt;/a&gt;. Want to monitor I/O of your disks? Try &lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/tools/biotop.py&#34;&gt;&lt;code&gt;biotop.py&lt;/code&gt;&lt;/a&gt;. Brendan Gregg has a great diagram of where all the various tools appears here: &lt;a href=&#34;http://www.brendangregg.com/Perf/linux_observability_tools.png&#34;&gt;http://www.brendangregg.com/Perf/linux_observability_tools.png&lt;/a&gt;&lt;/p&gt;
&lt;aside&gt;Addendum (Feb 2019): see my addendum above about &lt;code&gt;usdt&lt;/code&gt; hooks in Python and check out &lt;a href=&#34;https://github.com/benfred/py-spy&#34;&gt;&lt;code&gt;py-spy&lt;/code&gt;&lt;/a&gt; instead!&lt;/aside&gt;
&lt;p&gt;In addition to being written in Python, BCC ships with a tools that are useful for instrumenting Python applications. If you have ever tried to profile a Python application you may have tried &lt;a href=&#34;https://docs.python.org/3.6/library/profile.html&#34;&gt;&lt;code&gt;cProfile&lt;/code&gt;&lt;/a&gt;. But it has a performance impact on the application and you can&#39;t add it to a running production application after the fact. Instead you can use the &lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/tools/lib/ucalls.py&#34;&gt;&lt;code&gt;ucalls.py&lt;/code&gt;&lt;/a&gt; library (or its handy &lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/tools/pythoncalls.sh&#34;&gt;&lt;code&gt;pythoncalls&lt;/code&gt;&lt;/a&gt; wrapper). This hooks the usdt endpoints that we made sure our Python interpreter had when we built it earlier with the &lt;code&gt;--with-dtrace&lt;/code&gt; flag. Here we use it on a Django application that makes calculations via &lt;code&gt;numpy&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo /usr/share/bcc/tools/pythoncalls 30695
Tracing calls in process 30695 (language: python)... Ctrl-C to quit.
^C
METHOD                                                                  # CALLS
&amp;lt;frozen importlib._bootstrap_external&amp;gt;.__init__                               1
/srv/venv/api/lib/python3.6/site-packages/django/vi._EnsureCsrfToken          1
/srv/venv/api/lib/python3.6/site-packages/django/co.get_path_info             1
/srv/venv/api/lib/python3.6/site-packages/numpy/lib.poly1d                    1
/srv/venv/api/lib/python3.6/collections/__init__.py.update                    1
/srv/venv/api/lib/python3.6/site-packages/numpy/lib.DummyArray                1
/srv/venv/api/lib/python3.6/site-packages/numpy/lib.vectorize                 1
/srv/venv/api/lib/python3.6/site-packages/django/te.__init__                  1
/usr/local/lib/python3.6/logging/__init__.py._checkLevel                      1
/srv/venv/api/lib/python3.6/site-packages/numpy/cor.&amp;lt;listcomp&amp;gt;                1
/srv/venv/api/lib/python3.6/site-packages/numpy/lin._determine_error_states   1
/srv/venv/api/lib/python3.6/site-packages/numpy/lib.ConverterLockError        1
/srv/venv/api/lib/python3.6/site-packages/numpy/lib._set_function_name        1
/srv/venv/api/lib/python3.6/site-packages/numpy/ma/.mr_class                  1
/srv/venv/api/lib/python3.6/site-packages/numpy/cor._typedict                 1
/srv/venv/api/lib/python3.6/site-packages/numpy/ma/._convert2ma               1
/srv/venv/api/lib/python3.6/site-packages/numpy/lib.deprecate                 1
/usr/local/lib/python3.6/unittest/case.py._Outcome                            1
/srv/venv/api/lib/python3.6/enum.py.__and__                                   1
/srv/venv/api/lib/python3.6/site-packages/django/ut.find_module               1
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In addition to &lt;code&gt;pythoncalls&lt;/code&gt;, there&#39;s &lt;code&gt;pythonflow&lt;/code&gt; to trace execution flow, &lt;code&gt;pythongc&lt;/code&gt; to summarize garbage collection events, and &lt;code&gt;pythonstat&lt;/code&gt; to collect counts of exceptions, imports, or method calls. (These are actually all wrappers around a library of &lt;code&gt;usdt&lt;/code&gt;-reading tools that work for Python, Ruby, Java, or PHP.)&lt;/p&gt;
&lt;p&gt;Happy debugging!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Hotdog is Not a Sandwich</title>
      <link>https://blog.0x74696d.com/posts/a-hotdog-is-not-a-sandwich/</link>
      <pubDate>Wed, 02 May 2018 01:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/a-hotdog-is-not-a-sandwich/</guid>
      <description>&lt;p&gt;I was recently visiting lovely Austin, Texas. During a dinner with some friends, one of the non-techy folks at the table was getting hopelessly bored by our yammering on about blockchains or whatever, so I changed the subject to something more fun: the important debate over whether a hotdog is a sandwich.&lt;/p&gt;
&lt;p&gt;On the one hand this is just a silly meme, but it&#39;s also a convenient social shortcut. You get to learn about how someone thinks about problems, how they debate, something about their cultural context, if they have a sense of humor, if they&#39;re pompous windbags, or if they&#39;re the sort of person who corrects &amp;quot;less than&amp;quot; to &amp;quot;fewer&amp;quot; (but I repeat myself). More importantly, there are no stakes. No one&#39;s career, business, or identity is tied up in whether a hotdog is a sandwich.&lt;/p&gt;
&lt;p&gt;The hotdog debate also serves as a shortcut for talking about how we in the tech industry use language.&lt;/p&gt;
&lt;h2 id=&#34;precision&#34;&gt;Precision&lt;/h2&gt;
&lt;p&gt;You&#39;ve probably heard the adage &amp;quot;there are two hard things in computer science: naming things and cache invalidation.&amp;quot; When we write software, names are important to how we communicate our intent. Whole chapters of classic books like Steve McConnell&#39;s &lt;em&gt;Code Complete&lt;/em&gt; are devoted to naming things. Some folks go so far as suggest you shouldn&#39;t comment software because names of your types and functions should make everything obvious. (I think that sounds nice for greenfield web applications but maybe not so much for systems software where you might need a long &lt;a href=&#34;https://github.com/joyent/illumos-joyent/blob/master/usr/src/uts/common/io/mac/mac_sched.c#L29&#34;&gt;essay&lt;/a&gt; to explain the design to future generations.) We mostly all agree on the importance of names and definitions in that narrow context.&lt;/p&gt;
&lt;p&gt;Where we get into trouble is when we try to apply that precision more broadly. To paraphrase the folk singer &lt;a href=&#34;https://en.wikipedia.org/wiki/Utah_Phillips&#34;&gt;Utah Phillips&lt;/a&gt;, language is like a river. We put our concepts and experiences into the river and they flow away from us, until they no longer have our identity  they have their own utility. And others can borrow those notions to use in their own context. Language bridges the gap between me and another person, but it&#39;s also a source of confusion because words get muddied in the river.&lt;/p&gt;
&lt;p&gt;Most people quickly figure out that in order to say a hotdog is not a sandwich, that you need to have shared definitions of &amp;quot;hotdog&amp;quot; and &amp;quot;sandwich&amp;quot; in the first place. But the precision of that definition is only needed when you&#39;re trying to differentiate. If we&#39;re standing next to a table with a hoagie, a glass, and a fork, and I say to you &amp;quot;please pass me that sandwich,&amp;quot; you know exactly what my intent is even if you don&#39;t think a hoagie is really a sandwich. You&#39;re not going to be suddenly confused and hand me the fork. Even if you&#39;re from Europe or Asia and have no idea what the hell a hoagie is (or a hotdog for that matter), you&#39;re probably pretty sure from the context I meant the sandwich-looking thing.&lt;/p&gt;
&lt;p&gt;It&#39;s also much harder to agree on exact definitions when the concept being discussed is abstract. In our hotdog case, you can generate vigorous debate on the definition of something as concrete as a sandwich! The more abstract the concept, the less likely it is that you&#39;re going to have universal agreement on it. &amp;quot;What is a hotdog?&amp;quot; Ok. &amp;quot;What is a sandwich?&amp;quot; Hm. &lt;a href=&#34;https://www.youtube.com/watch?v=HEXWRTEbj1I&#34;&gt;&amp;quot;What is love?&amp;quot;&lt;/a&gt; Uh...&lt;/p&gt;
&lt;p&gt;The important thing in any given conversation then is not some canonical definition of a word, but the definition that&#39;s being shared (by understanding or even just temporary agreement) by the folks in that conversation. Where this gets tricky is when words are tied up in agendas and identity.&lt;/p&gt;
&lt;h2 id=&#34;agendas&#34;&gt;Agendas&lt;/h2&gt;
&lt;p&gt;The hotdog industry probably doesn&#39;t care whether anyone thinks a hotdog is a sandwich. I can&#39;t see this being a matter of intense debate in the boardroom at Hofmann&#39;s. If anything they probably love the meme just because it gets people thinking about hotdogs.&lt;/p&gt;
&lt;p&gt;But in our industry there are lots of folks who do have agendas around definitions. The source of this is mostly an attempt at differentiation. Imagine you are an artisanal butcher making &lt;a href=&#34;https://en.wikipedia.org/wiki/White_hot&#34;&gt;coneys&lt;/a&gt; instead of hotdogs. You need to stand out from vast crowd of folks making hotdogs. You don&#39;t want your product to be confused for hotdogs, particularly because if someone is coming to you for a hotdog they&#39;re going to have a novel and amazing experience but maybe not what they thought they wanted. Maybe they&#39;ll then turn around and say to other prospects &amp;quot;they make weird hotdogs.&amp;quot; Disaster, right?&lt;/p&gt;
&lt;p&gt;I once worked with a team that had this &amp;quot;underdog syndrome.&amp;quot; Because they were a small team their offering wasn&#39;t as feature rich as the leading competitors. But instead of focusing on their strengths, a lot of their content marketing efforts were focused on playing games with definitions. On one hand you can try to parlay this into attention for your organization, and that can work. But it can also result in you having public disagreements with competitors about the definition of &amp;quot;standards&amp;quot; or &amp;quot;monitoring&amp;quot;, which makes everyone involved seem out of touch.&lt;/p&gt;
&lt;p&gt;The worst case of this is when a team tries to take an existing term of art that&#39;s broadly agreed-upon and tries to forcibly change the definition to match what they do. I consulted recently for a team that&#39;s a bit disconnected from the wider industry that was trying to redefine things like &amp;quot;PKI&amp;quot; and &amp;quot;serverless.&amp;quot; This was not going to go well!&lt;/p&gt;
&lt;p&gt;Your customers don&#39;t care about your definitions. They care about &lt;em&gt;their&lt;/em&gt; definitions! Yes, it&#39;s sometimes important to educate your customers to make them more successful. But creating distance between vendor and customer isn&#39;t what you want  you have to reach them where they are.&lt;/p&gt;
&lt;h2 id=&#34;identity&#34;&gt;Identity&lt;/h2&gt;
&lt;p&gt;The most hard-to-shake agenda is personal identity. If your identity is tied up in a particular definition it makes it much harder to bridge the gap of understanding and have a conversation about the underlying concept. You&#39;ll need to bring everyone to your particular definition first. If you find yourself doing this a lot, step back and examine whether you intended to make that definition part of your personal identity. If someone is being particular about definitions for no obvious reason, step back and consider whether you&#39;re threatening their sense of identity in the process.&lt;/p&gt;
&lt;p&gt;Suppose one day the folks who grill hotdogs for a living came to the butchers who make hotdogs and said &amp;quot;if we work more closely together we can produce a better tasting-hotdog more quickly, and with less stress and more money for both of us.&amp;quot; And the teams who got really into this decided to call it ButcherGrills, a marrying of the making of hotdogs with their delivery. Pretty cool, right? Well at some point the large hotdog manufacturers see this and say to themselves &amp;quot;we need this ButcherGrills stuff, can we hire people to do it?&amp;quot; And so they start hiring people and give them the title ButcherGrills, because if they hire grillers or butchers who don&#39;t know about the ButcherGrills methodologies they&#39;ll have to train them and as big companies they&#39;d rather die than ever train anyone to do anything.&lt;/p&gt;
&lt;p&gt;Now we&#39;re in a situation where there are people who are &lt;em&gt;doing&lt;/em&gt; ButcherGrills and another group of people who are calling &lt;em&gt;themselves&lt;/em&gt; ButcherGrills. The second group is never going to agree with the first group because their career is built around being &amp;quot;a ButcherGrills&amp;quot;. Not to mention that ButcherGrills roles seem to pay a lot better than griller roles.&lt;/p&gt;
&lt;p&gt;What is to be done? We can try to play with definitions some more; we need a new category of folks who are Hotdog Grillability Engineers. Maybe that&#39;s useful if it adds anything new to the discussion instead of being co-opted to mean the same thing that ButcherGrills did. But is the definition all that important? Or is it only important that we share &lt;em&gt;some&lt;/em&gt; definition with the folks we&#39;re communicating with?&lt;/p&gt;
&lt;p&gt;More importantly, what&#39;s it to me? If someone&#39;s identity is tied up in a definition that means it&#39;s important to them. Unless I have my identity tied up in a different definition, it literally costs me &lt;em&gt;nothing&lt;/em&gt; to accept someone where they are. (And if I do have my identity tied up in a different definition, it&#39;s on me to examine myself about whether this definition is healthy, whether it&#39;s tied up in ego or privilege, etc.) If someone says that their title is ButcherGrills, who am I to say &amp;quot;well that&#39;s not what ButcherGrills really is&amp;quot;?&lt;/p&gt;
&lt;p&gt;The most important definitions for us to agree upon are the ones that folks assign to themselves. Whether someone self-identifies as a sysadmin, devops engineer, or SRE; whether they self-identify as a he, her, or they; or whether they self-identify as working in observability, monitoring, or logging; we start from a better place of empathy and understanding if we take those identities as a given and work together from there.&lt;/p&gt;
&lt;p&gt;But a hotdog is still not a sandwich.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Software Defined Culture, Part 4 - Responsibility</title>
      <link>https://blog.0x74696d.com/posts/software-defined-culture-4-responsibility/</link>
      <pubDate>Sun, 18 Feb 2018 04:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/software-defined-culture-4-responsibility/</guid>
      <description>&lt;p&gt;This five-part series of posts covers a talk titled &lt;em&gt;Software Defined Culture&lt;/em&gt; that I gave at &lt;a href=&#34;https://www.devopsdays.org/events/2016-philadelphia/program/tim-gross/&#34;&gt;DevOps Days Philadelphia 2016&lt;/a&gt;, &lt;a href=&#34;https://gotochgo.com/2017/sessions/43&#34;&gt;GOTO Chicago 2017&lt;/a&gt;, and &lt;a href=&#34;https://vimeo.com/228067673&#34;&gt;Velocity San Jose 2017&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you&#39;d like to read the rest of the series:&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture/&#34;&gt;Part 0: Software Defined Culture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-1-reliability/&#34;&gt;Part 1: Build for Reliability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-2-operability/&#34;&gt;Part 2: Build for Operability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-3-observability/&#34;&gt;Part 3: Build for Observability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Part 4: Build for Responsibility&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id=&#34;building-for-responsibility&#34;&gt;Building for Responsibility&lt;/h1&gt;
&lt;p&gt;This has been the section of the talk where I definitely lost some of the audience. In fairness, you were warned at the beginning that we were going to talk about culture and that it can be tricky. So if you&#39;ve been following along so far and don&#39;t think I&#39;m totally crazy yet... hold on to your butts!&lt;/p&gt;
&lt;h2 id=&#34;every-tool-comes-with-a-community&#34;&gt;Every Tool Comes With a Community&lt;/h2&gt;
&lt;p&gt;All our technical choices implicitly make us a part of the community of users of the tools we choose. Some of those communities have been intentionally and thoughtfully designed around making sure that everyone can participate in the community. The Rust language community is an amazing example of this. From the very beginning of their organization, they realized that the technical community would be stronger when everyone takes responsibility towards making it inclusive. From their &lt;a href=&#34;https://www.rust-lang.org/en-US/conduct.html&#34;&gt;Code of Conduct&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Even if you feel you were misinterpreted or unfairly accused, chances are good there was something you could&#39;ve communicated better  remember that it&#39;s your responsibility to make your fellow Rustaceans comfortable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now, I&#39;m not saying you should go and switch all your production code to Rust. I did recommend in Part 1 to choose boring technologies, after all! But it&#39;s clear that the Rust community has taken seriously its responsibility to be welcoming to all.&lt;/p&gt;
&lt;p&gt;Other communities have well-deserved reputations for toxicity. When you choose a technology where a portion of the community forked the runtime in protest of the steward company&#39;s CTO defense of gender-neutral pronouns in documentation, you&#39;re implicitly saying you&#39;re okay with being part of that community. Maybe that&#39;s the right choice because the rest of the community outweighs the sexist assholes. But we should make those trade-offs with open eyes.&lt;/p&gt;
&lt;p&gt;When you choose a language where the original developers defend their language design decisions because their co-workers are &lt;a href=&#34;https://channel9.msdn.com/Events/Lang-NEXT/Lang-NEXT-2014/From-Parallel-to-Concurrent&#34;&gt;&amp;quot;not capable of understanding a brilliant language but we want to use them to build good software&amp;quot;&lt;/a&gt;, then you shouldn&#39;t be surprised if this mentality has trickled into the community.&lt;/p&gt;
&lt;p&gt;When we choose a technology for our organization, it determines what communities we&#39;ll be a part of. It determines who we&#39;ll hire. If your organization&#39;s mission is to build amazing compiler tools, then you might want to hire people who know that &amp;quot;a monad is a monoid in the category of endofunctor&amp;quot; (not me!). But if you&#39;re making line-of-business CRUD web apps that mostly serialize rows in and out of a database, you might not want to choose Haskell. Not because of any technical constraints, but because the community of developers you&#39;ll have access to are going to be bored out of their minds writing line-of-business CRUD web apps.&lt;/p&gt;
&lt;h2 id=&#34;community-doesnt-stop-with-your-team&#34;&gt;Community Doesn&#39;t Stop With Your Team&lt;/h2&gt;
&lt;p&gt;Our technical communities are only one part of building with responsibility in mind. There is also the larger community  our culture at large. The culture impact of our technical choices reflect our biases and they reflect our blind spots. When Google and Facebook demand the legal names of users, it&#39;s because they&#39;re not considering the risks that political activists, abuse victims, or trans persons would be taking by giving this information up. When a media website pushes down 10MB of crummy ads to go along with 100 words of news content, it&#39;s because they&#39;re not considering the impact this has on working class users with low-end limited data plans, who can&#39;t afford the stuff in those crummy ads anyways.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/polotek/status/856183297180704768&#34;&gt;&lt;img src=&#34;https://blog.0x74696d.com/images/20180218/marco-rogers.png&#34; alt=&#34;Kalanick keeps asking for unethical/illegal things, but at some point we have to talk about how engineers at Uber keep saying yes.&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When I gave this talk, I asked the audience to stop and reflect on what they were doing on a day to day basis. Am I making the world worse? This can be for whatever definition of &amp;quot;worse&amp;quot; you want to use. Maybe you&#39;re ok with ads, but not pharmaceutical ads. Maybe you&#39;re ok making embedded software for sensors but not for weapons. Maybe you&#39;re ok making weapons, so long as the software that delivers them is Free and Open Source. But whatever your definition is, you can ask yourself this question.&lt;/p&gt;
&lt;p&gt;I then ask how many people work for organizations that are hiring. In a packed room of hundreds of people, virtually every hand goes up. If you think you&#39;re making the world worse, the market conditions in our industry are such that you don&#39;t have to continue to participate in that. Find somewhere that doesn&#39;t make the world worse.&lt;/p&gt;
&lt;h2 id=&#34;professional-ethics&#34;&gt;Professional Ethics&lt;/h2&gt;
&lt;p&gt;The software industry doesn&#39;t have a professional licensing body. That&#39;s probably the right call, at least for now. There&#39;s so much variation in what we do  the people who write web apps and the people who build embedded systems for airplanes are only barely in the same profession. But other industries do have licensing bodies, and I think we can learn a thing or two from our siblings in other professions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Engineers shall hold paramount the safety, health and welfare of the public and shall strive to comply with the principles of sustainable development... Engineers shall act in such a manner as to uphold and enhance the honor, integrity, and dignity of the engineering profession.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is from the &lt;a href=&#34;http://www.asce.org/code-of-ethics/&#34;&gt;Code of Ethics for the American Society of Civil Engineers&lt;/a&gt;. Imagine what the software industry would look like if we held to this code of ethics. Imagine if we had &lt;a href=&#34;https://www.youtube.com/watch?v=9QMGAtxUlAc&#34;&gt;shared principles&lt;/a&gt;. Imagine how it would change the impact we have on the world. We don&#39;t need a governing body to impose this on us.&lt;/p&gt;
&lt;p&gt;We can start today.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;If you&#39;d like to read the rest of the series:&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture/&#34;&gt;Part 0: Software Defined Culture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-1-reliability/&#34;&gt;Part 1: Build for Reliability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-2-operability/&#34;&gt;Part 2: Build for Operability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-3-observability/&#34;&gt;Part 3: Build for Observability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Part 4: Build for Responsibility&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Software Defined Culture, Part 3 - Observability</title>
      <link>https://blog.0x74696d.com/posts/software-defined-culture-3-observability/</link>
      <pubDate>Sun, 18 Feb 2018 03:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/software-defined-culture-3-observability/</guid>
      <description>&lt;p&gt;This five-part series of posts covers a talk titled &lt;em&gt;Software Defined Culture&lt;/em&gt; that I gave at &lt;a href=&#34;https://www.devopsdays.org/events/2016-philadelphia/program/tim-gross/&#34;&gt;DevOps Days Philadelphia 2016&lt;/a&gt;, &lt;a href=&#34;https://gotochgo.com/2017/sessions/43&#34;&gt;GOTO Chicago 2017&lt;/a&gt;, and &lt;a href=&#34;https://vimeo.com/228067673&#34;&gt;Velocity San Jose 2017&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you&#39;d like to read the rest of the series:&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture/&#34;&gt;Part 0: Software Defined Culture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-1-reliability/&#34;&gt;Part 1: Build for Reliability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-2-operability/&#34;&gt;Part 2: Build for Operability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Part 3: Build for Observability&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-4-responsibility/&#34;&gt;Part 4: Build for Responsibility&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id=&#34;building-for-observability&#34;&gt;Building for Observability&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;We have built mind-bogglingly complicated systems that we cannot see, allowing glaring performance problems to hide in broad daylight in our systems.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Bryan Cantrill, CTO of Joyent, said this back in 2006 in &lt;a href=&#34;http://queue.acm.org/detail.cfm?id=1117401&#34;&gt;ACM Queue&lt;/a&gt;. And that was more than ten years ago! Turns out we were all building distributed systems back then, but now we&#39;ve all embraced that we&#39;re building distributed systems, and these systems make the situation even harder.&lt;/p&gt;
&lt;p&gt;In a distributed system, the gnarliest and most difficult problems to solve will only appear in production. This means we need to be able to understand what&#39;s happening in our production systems. They need to be observable, and the tools we use to obtain that observability must above all be &lt;em&gt;safe&lt;/em&gt; to use in production.&lt;/p&gt;
&lt;p&gt;But that&#39;s not all. In his &lt;a href=&#34;https://www.kitchensoap.com/2015/05/01/openlettertomonitoringproducts/&#34;&gt;&lt;em&gt;Open Letter to Monitoring/Alerting Companies&lt;/em&gt;&lt;/a&gt;, John Allspaw says:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;[T]ake as a first design principle that outages and other &amp;quot;untoward&amp;quot; events are handled not by a lone engineer, but more often than not by a team of engineers all with their different expertise and focus of attention.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That is, the tools that we&#39;re building should be collaborative. Don&#39;t simply create dashboards showing the metrics associated with your previous outage. Select tools that allow you to debug &lt;em&gt;in situ&lt;/em&gt; like DTrace and eBPF, and let you write code that can be committed and shared as part of the learning process. Select tools like &lt;a href=&#34;https://honeycomb.io/&#34;&gt;Honeycomb&lt;/a&gt;, that allow you to iteratively build queries of events emitted by your system, which you can then share as playbooks for your next incident.&lt;/p&gt;
&lt;h2 id=&#34;observability-as-a-first-class-requirement&#34;&gt;Observability as a First-Class Requirement.&lt;/h2&gt;
&lt;p&gt;I once had responsibility for a Windows Distributed File System Replication (DFSR) cluster. In case you&#39;re not familiar, this is block storage distributed over the WAN (what could &lt;em&gt;possibly&lt;/em&gt; go wrong?). This was many years ago before I&#39;d won my production battle scars, so when we selected the system we didn&#39;t really take into consideration how we could observe its operation. When our users started reporting bizarre file locking behavior (&lt;em&gt;gasp who could&#39;ve thought!?&lt;/em&gt;), we realized that DFSR had no way to tell us what it was doing. The best we could get out of it was a report that said &amp;quot;here&#39;s how much bandwidth I&#39;ve saving in compression,&amp;quot; which was not very helpful. We went through the 5 Stages of Observability Grief:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Denial: &amp;quot;We don&#39;t really need to worry about monitoring this, right?&amp;quot;&lt;/li&gt;
&lt;li&gt;Anger: &amp;quot;Why can&#39;t we monitor this?!&amp;quot;&lt;/li&gt;
&lt;li&gt;Bargaining: &amp;quot;Microsoft, surely you have a tool to observe this... can we have yours?&amp;quot;&lt;/li&gt;
&lt;li&gt;Depression: &amp;quot;I&#39;ve been on the phone for 2 months with Microsoft... will this never end?&amp;quot;&lt;/li&gt;
&lt;li&gt;&amp;quot;Fuck this, we&#39;ll build our own tools!&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I built our own monitoring tooling based on Window Management Instrumentation (WMI), which for a small engineering firm (rails and runways, not software) with a two person tech team was bit of a lift. This project ended up driving me to greater participation in Philadelphia Python Users Group (PhillyPUG), giving talks on debugging Python, and eventually my first serious ops role at DramaFever.&lt;/p&gt;
&lt;p&gt;What does this charming origin story have to do with culture? Because we didn&#39;t have a strong culture of observability as a first-class requirement, we ended up burning a lot of time and energy in building our own tooling. Taking ownership of our observability empowered us to make better technical decisions in the future. It&#39;s also a cautionary tale for culture; if you&#39;re an organization that has a hard time in taking ownership of its own tooling, you may lose team members to organizations that don&#39;t.&lt;/p&gt;
&lt;h2 id=&#34;debugability&#34;&gt;Debugability&lt;/h2&gt;
&lt;p&gt;The decisions you can make to improve observability take place at every level of the stack, from deployment platform choices all the way down to build flags.&lt;/p&gt;
&lt;p&gt;If you&#39;re stripping your production binaries or passing &lt;code&gt;--fomit-frame-pointer&lt;/code&gt; to your compiler, you&#39;re making tradeoffs around your ability to easily observe what&#39;s happening in your applications. To take a real-world example from Joyent, it&#39;s the difference between having a flame graph that says &amp;quot;well the problem is somewhere here in third-party code&amp;quot; and a flame graph that says &amp;quot;here&#39;s the exact part of the algorithm that&#39;s causing the slowdown, and we can improve the performance by switching from RSA to ECDSA.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.0x74696d.com/images/20180218/flamegraph-no-framepointers.png&#34; alt=&#34;Flame graph without frame pointers&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.0x74696d.com/images/20180218/flamegraph-with-framepointers.png&#34; alt=&#34;Flame graph with frame pointers&#34;&gt;&lt;/p&gt;
&lt;p&gt;If you&#39;re looking at those flame graphs and saying &amp;quot;no one in my organization even knows how to do that&amp;quot;, you should probably hire someone who does. And if you&#39;re looking to &amp;quot;level up&amp;quot; your development skills as an intermediate developer, you would be well-served by learning how to profile at this level.&lt;/p&gt;
&lt;h2 id=&#34;platform-choices&#34;&gt;Platform Choices&lt;/h2&gt;
&lt;p&gt;If you deploy onto a platform where you don&#39;t have root and can&#39;t even do something like start a debugger, run &lt;code&gt;perf&lt;/code&gt;, or generate a flame graph? Well, I&#39;m not telling you that you should never use Google App Engine or Heroku or Elastic Beanstalk, but you should definitely understand what you&#39;re giving up.&lt;/p&gt;
&lt;p&gt;This extends to the choice of programming language as well. If your &lt;a href=&#34;https://golang.org/doc/gdb&#34;&gt;language documentation&lt;/a&gt; tells developers that debugging isn&#39;t a priority, what does this say about the culture of debugging?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;GDB does not understand Go programs well... it is not a reliable debugger for Go programs, particularly heavily concurrent ones. Moreover, it is not a priority for the Go project to address these issues, which are difficult.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the case of golang, third parties have stepped up and (somewhat) improved the situation, but most of the stdlib profiling tools have been intermittently broken on non-Linux platforms for years (you can see an example in my &lt;a href=&#34;https://blog.0x74696d.com/posts/be-careful-what-you-benchmark/&#34;&gt;&lt;em&gt;Be Careful What You Benchmark&lt;/em&gt;&lt;/a&gt; post).&lt;/p&gt;
&lt;p&gt;This isn&#39;t an intractable situation. If you think a language has a lot of other things going for it, you can invest in building better observability tooling for it. Joyent has famously done so with Node.js, and as they are adopting more golang, one of the first projects they&#39;ve embarked on is improving their ability to debug golang software.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/jen20/status/853943464131780608&#34;&gt;&lt;img src=&#34;https://blog.0x74696d.com/images/20180218/dtrace-tweet.png&#34; alt=&#34;DTrace PID provider FBT for Go (with arguments) on SmartOS! In the second run we match on an arg value, stop, take a core dump and resume.&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.0x74696d.com/images/20180218/dtrace-golang.png&#34; alt=&#34;DTrace golang&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;culture-of-observability&#34;&gt;Culture of Observability&lt;/h2&gt;
&lt;p&gt;Simply having access to good tooling for observability doesn&#39;t get you much in the way of culture change. You have to use that tooling! If you only make a point of using your ability to observe your system when things are going very wrong, you won&#39;t have built up the skills to use them well. Moreover, as Charity Majors points out in &lt;a href=&#34;https://honeycomb.io/blog/2016/10/part-5/5-building-badass-engineers-and-badass-teams/&#34;&gt;&lt;em&gt;Building Badass Engineers and Badass Teams&lt;/em&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Get used to interacting with your observability tooling every day. As part of your release cycle, or just out of curiosity. Honestly, things are broken all the time - you dont even know what normal looks like unless youre also interacting with your observability tooling under &amp;quot;normal&amp;quot; circumstances.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Making decisions that keep observability as a first class citizen aren&#39;t just important from a technical standpoint. The concept of observability applies to every aspect of an organization&#39;s operation. Being able to understand the impact of our behaviors is the only way to keep ourselves honest. This applies to everything from deploying software, to marketing campaigns, to making HR policy changes. Observability is the first requirement to becoming a &lt;a href=&#34;https://www.youtube.com/watch?v=IdZaFzuOPUQ&#34;&gt;learning organization&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;If you&#39;d like to read the rest of the series:&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture/&#34;&gt;Part 0: Software Defined Culture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-1-reliability/&#34;&gt;Part 1: Build for Reliability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-2-operability/&#34;&gt;Part 2: Build for Operability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Part 3: Build for Observability&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-4-responsibility/&#34;&gt;Part 4: Build for Responsibility&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Software Defined Culture, Part 2 - Operability</title>
      <link>https://blog.0x74696d.com/posts/software-defined-culture-2-operability/</link>
      <pubDate>Sun, 18 Feb 2018 02:00:00 +0000</pubDate>
      
      <guid>https://blog.0x74696d.com/posts/software-defined-culture-2-operability/</guid>
      <description>&lt;p&gt;This five-part series of posts covers a talk titled &lt;em&gt;Software Defined Culture&lt;/em&gt; that I gave at &lt;a href=&#34;https://www.devopsdays.org/events/2016-philadelphia/program/tim-gross/&#34;&gt;DevOps Days Philadelphia 2016&lt;/a&gt;, &lt;a href=&#34;https://gotochgo.com/2017/sessions/43&#34;&gt;GOTO Chicago 2017&lt;/a&gt;, and &lt;a href=&#34;https://vimeo.com/228067673&#34;&gt;Velocity San Jose 2017&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you&#39;d like to read the rest of the series:&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture/&#34;&gt;Part 0: Software Defined Culture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-1-reliability/&#34;&gt;Part 1: Build for Reliability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Part 2: Build for Operability&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-3-observability/&#34;&gt;Part 3: Build for Observability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-4-responsibility/&#34;&gt;Part 4: Build for Responsibility&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id=&#34;building-for-operability&#34;&gt;Building for Operability&lt;/h1&gt;
&lt;p&gt;For purposes of this section, I&#39;m talking about operability as the ability for teams to deploy and operate their software well. Ideally this should be in some kind of self-service way, where developers don&#39;t need to go through some other team (&amp;quot;operators&amp;quot;) to deploy software.&lt;/p&gt;
&lt;p&gt;One of the notional frameworks that has popped up around this in the last few years is GIFEE (&amp;quot;Google Infrastructure for Everyone Else&amp;quot;). This notion has largely reified itself in the last year especially in Kubernetes and the hodge-podge of vaguely related projects under the banner of the Cloud Native Computing Foundation (CNCF).&lt;/p&gt;
&lt;p&gt;But &amp;quot;Google does it this way, so should we&amp;quot; suggests that you have similar problems to Google. Spoiler alert: this is unlikely to be the case. I&#39;ve talked with teams at some of the largest retailers in the world and discovered giant e-commerce properties fitting in the equivalent of a handful of racks. Your startup (and mine!) is a tiny fraction of the scale, so why would we expect the solutions to be the same?&lt;/p&gt;
&lt;p&gt;Orchestrators like Kubernetes are designed to handle a wide diversity of organizational requirements, and this is reflected in a huge amount of choice (the diversity of networking plugins alone!) that becomes incumbent upon the operators to handle. Cindy Sridharan&#39;s &lt;a href=&#34;https://medium.com/@copyconstruct/schedulers-kubernetes-and-nomad-b0f2e14a896&#34;&gt;excellent blog post&lt;/a&gt; from last summer dives into the choices her organization made around Kubernetes versus a less complex scheduler like Nomad.&lt;/p&gt;
&lt;h2 id=&#34;whos-complexity&#34;&gt;Who&#39;s Complexity?&lt;/h2&gt;
&lt;p&gt;Most engineers are familiar with the concept of essential vs incidental complexity, but perhaps less commonly understood is how the &amp;quot;essentialness&amp;quot; of complexity is deeply tied to ones perspective. &lt;em&gt;The complexity of Kubernetes is essential complexity from the perspective of Kubernetes-the-project, but it is incidental complexity from the perspective of your organization.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The problem doesn&#39;t quite end at the orchestration layer. There has been a strong trend over the last few years towards pushing &amp;quot;intelligence&amp;quot; out of the application and into infrastructure components. The narrative is that application developers shouldn&#39;t have to worry about concerns like service discovery, tracing, failover, configuration, etc. and that they should be solely focused on &amp;quot;business logic.&amp;quot; I&#39;ve been told 2018 is The Year of the Service Mesh, for example.&lt;/p&gt;
&lt;p&gt;Whether this trend has been exacerbated by the large number of VC-backed infrastructure startups who have a vested interest in this being the prevailing narrative is left as an exercise for the reader. But in addition to reducing application developers to line-of-business specialists ready to be washed away in the next wave of Taylorist automation, this leads to some serious problems when it comes to running applications in production.&lt;/p&gt;
&lt;p&gt;If the application behavior has been abstracted away from its environment, this means the application developer can&#39;t understand the real-world behavior of their application without running it on the platform either. It&#39;s a reincarnation of RPC by remote function call; the application developer can&#39;t really treat the infrastructure like an abstraction. The application developer can&#39;t really pretend that a database cluster is sitting at localhost when there are application-specific semantics to how it behaves when replication degrades. This just leads to &amp;quot;works on my machine&amp;quot; and we&#39;re back to the same problem we were trying to solve with all our new fancy orchestration tools in the first place!&lt;/p&gt;
&lt;h2 id=&#34;self-operating-applications&#34;&gt;Self-Operating Applications&lt;/h2&gt;
&lt;p&gt;What&#39;s the alternative? While I was at Joyent I worked on a project called &lt;a href=&#34;https://github.com/joyent/containerpilot&#34;&gt;ContainerPilot&lt;/a&gt;, along with design patterns that we collectively called the Autopilot Pattern. The concept of the Autopilot Pattern was that the application should be responsible for its own lifecycle as much as possible. Once deployed by a (minimal) orchestration platform, applications can find the service discovery database, gossip their configuration, elect leaders, and trigger events in their own lifecycle. ContainerPilot was envisioned as a container init system that would help bridge the gap to these behaviors for legacy applications. The Joyent folks have continued on with the project after my departure, but I&#39;ve seen it at work successfully at large enterprise retailers and startups alike.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.habitat.sh/&#34;&gt;Habitat&lt;/a&gt; project by Chef is another example of this same philosophy at work. Habitat goes a step further by owning the entire build process for the application container as well. By packaging the application and its automation together, you get consistent deployments, automated dependency awareness, and decentralized intelligence. But more importantly, you empower development teams to understand the production behavior of their applications.&lt;/p&gt;
&lt;p&gt;Google infrastructure is probably awesome but does it solve a problem we actually have? &amp;quot;GIIPABDISAPWAH&amp;quot; is less catchy than &amp;quot;GIFEE&amp;quot;, I&#39;ll admit. Building for operability, as described here, builds a culture of trust (there&#39;s that word again) between developers and operators, a culture of empowerment for your developers, and a culture with fewer silos.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;If you&#39;d like to read the rest of the series:&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture/&#34;&gt;Part 0: Software Defined Culture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-1-reliability/&#34;&gt;Part 1: Build for Reliability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Part 2: Build for Operability&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-3-observability/&#34;&gt;Part 3: Build for Observability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.0x74696d.com/posts/software-defined-culture-4-responsibility/&#34;&gt;Part 4: Build for Responsibility&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
